{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdde3ba",
   "metadata": {},
   "source": [
    "# Reading in and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38dc694d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>story_id</th>\n",
       "      <th>prompt_label</th>\n",
       "      <th>preset_label</th>\n",
       "      <th>sample</th>\n",
       "      <th>tss_coh_1</th>\n",
       "      <th>tss_coh_2</th>\n",
       "      <th>tss_coh_3</th>\n",
       "      <th>tss_coh_4</th>\n",
       "      <th>tss_coh_5</th>\n",
       "      <th>...</th>\n",
       "      <th>duration_in_sec</th>\n",
       "      <th>recorded</th>\n",
       "      <th>tss_order</th>\n",
       "      <th>qual_check_1</th>\n",
       "      <th>qual_check_1_order</th>\n",
       "      <th>qual_check_2</th>\n",
       "      <th>pass_qual_1</th>\n",
       "      <th>pass_qual_2</th>\n",
       "      <th>read_consesus</th>\n",
       "      <th>read_fre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R_3LZPTbqxmeWvp7w</td>\n",
       "      <td>GEN_HF_6</td>\n",
       "      <td>High Fantasy</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>Community</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1044</td>\n",
       "      <td>2022-02-17 07:58:47</td>\n",
       "      <td>I had a hard time making sense of what was goi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>88.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R_3CCMkj9T7UgOtgP</td>\n",
       "      <td>ALL_HR_3</td>\n",
       "      <td>Historical Romance</td>\n",
       "      <td>All-Nighter</td>\n",
       "      <td>Community</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>548</td>\n",
       "      <td>2022-02-17 08:01:20</td>\n",
       "      <td>I had a hard time making sense of what was goi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>89.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R_3PBKFhmDXlAQNO6</td>\n",
       "      <td>ALL_HOR_2</td>\n",
       "      <td>Horror</td>\n",
       "      <td>All-Nighter</td>\n",
       "      <td>Community</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>473</td>\n",
       "      <td>2022-02-17 08:03:16</td>\n",
       "      <td>I had a hard time making sense of what was goi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>99.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         response_id   story_id        prompt_label preset_label     sample  \\\n",
       "0  R_3LZPTbqxmeWvp7w   GEN_HF_6        High Fantasy      Genesis  Community   \n",
       "1  R_3CCMkj9T7UgOtgP   ALL_HR_3  Historical Romance  All-Nighter  Community   \n",
       "2  R_3PBKFhmDXlAQNO6  ALL_HOR_2              Horror  All-Nighter  Community   \n",
       "\n",
       "   tss_coh_1  tss_coh_2  tss_coh_3  tss_coh_4  tss_coh_5  ...  \\\n",
       "0        2.0        4.0        4.0        4.0        4.0  ...   \n",
       "1        2.0        4.0        2.0        1.0        2.0  ...   \n",
       "2        5.0        5.0        1.0        1.0        3.0  ...   \n",
       "\n",
       "   duration_in_sec             recorded  \\\n",
       "0             1044  2022-02-17 07:58:47   \n",
       "1              548  2022-02-17 08:01:20   \n",
       "2              473  2022-02-17 08:03:16   \n",
       "\n",
       "                                           tss_order  qual_check_1  \\\n",
       "0  I had a hard time making sense of what was goi...           NaN   \n",
       "1  I had a hard time making sense of what was goi...           NaN   \n",
       "2  I had a hard time making sense of what was goi...           NaN   \n",
       "\n",
       "   qual_check_1_order  qual_check_2  pass_qual_1  pass_qual_2  read_consesus  \\\n",
       "0                 NaN           NaN          NaN          NaN            8.0   \n",
       "1                 NaN           NaN          NaN          NaN            6.0   \n",
       "2                 NaN           NaN          NaN          NaN            6.0   \n",
       "\n",
       "   read_fre  \n",
       "0     88.77  \n",
       "1     89.99  \n",
       "2     99.77  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression as linreg\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures as polyfeat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "import factor_analyzer as fa\n",
    "import pingouin as pg\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import gspread as gs\n",
    "import gspread_dataframe as gd\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "tss_df = pd.read_csv(\"data/combined_data.csv\")\n",
    "items_descr = pd.read_csv(\"data/description_items.csv\")\n",
    "\n",
    "tss_df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "items_descr.set_index(\"Unnamed: 0\", drop=True, inplace=True)\n",
    "items_descr.index.set_names(\"item_label\", inplace=True)\n",
    "items_descr = pd.Series(items_descr.iloc[:, 0], name=\"item description\")\n",
    "\n",
    "tss_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcafa138",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "item_labels_li = [\n",
    "    'tss_coh_1', 'tss_coh_2', 'tss_coh_3', 'tss_coh_4',\n",
    "    'tss_coh_5', 'tss_coh_6', 'tss_coh_7', 'tss_coh_8', 'tss_coh_9',\n",
    "    'tss_coh_10', 'tss_coh_11', 'tss_coh_12', 'tss_conch_1', 'tss_conch_2',\n",
    "    'tss_conch_3', 'tss_conch_4', 'tss_conch_5', 'tss_conch_6',\n",
    "    'tss_conch_7', 'tss_conch_8', 'tss_cre_1', 'tss_cre_2', 'tss_cre_3',\n",
    "    'tss_cre_4', 'tss_cre_5', 'tss_cre_6', 'tss_cre_7', 'tss_cre_8',\n",
    "    'tss_cre_9', 'tss_cre_10', 'tss_cre_11', 'tss_cre_12', 'tss_qua_1',\n",
    "    'tss_qua_2', 'tss_qua_3', 'tss_qua_4', 'tss_qua_5', 'tss_qua_6',\n",
    "    'tss_qua_7', 'tss_qua_8', 'tss_rep_1', 'tss_rep_2', 'tss_rep_3',\n",
    "    'tss_rep_4', 'tss_rep_5', 'tss_rep_6', 'tss_rep_7', 'tss_rep_8',\n",
    "    'tss_rep_9', 'tss_rep_10', 'tss_rep_11', 'tss_rep_12', 'tss_sty_1',\n",
    "    'tss_sty_2', 'tss_sty_3', 'tss_sty_4', 'tss_sty_5', 'tss_sty_6',\n",
    "    'tss_sty_7', 'tss_sty_8', 'tss_sty_9', 'tss_sty_10', 'tss_sty_11',\n",
    "    'tss_sty_12', 'tss_pac_1', 'tss_pac_2', 'tss_pac_3', 'tss_pac_4',\n",
    "    'tss_pac_5', 'tss_pac_6', 'tss_pac_7', 'tss_pac_8', 'tss_pac_9'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9055dece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 cols with missing data per col:\n",
      "tss_pac_9     3\n",
      "tss_qua_2     3\n",
      "tss_rep_9     3\n",
      "tss_coh_2     3\n",
      "tss_cre_10    3\n",
      "dtype: int64\n",
      "\n",
      "Top 5 missing data per row:\n",
      "117    72\n",
      "137    65\n",
      "77     35\n",
      "0       0\n",
      "216     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(323, 95)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing data patterns\n",
    "missing = tss_df[item_labels_li].isna()\n",
    "print(\"Top 5 cols with missing data per col:\")\n",
    "print(missing.sum().sort_values(ascending=False).head(5))\n",
    "print(\"\\nTop 5 missing data per row:\")\n",
    "print(missing.sum(axis=1).sort_values(ascending=False).head(5))\n",
    "\n",
    "# in this case it is straightforward - just drop the 3 rows with missing data\n",
    "tss_df.dropna(subset=item_labels_li, inplace=True)\n",
    "\n",
    "# if missing data should be more common with small amounts of missing data per case, might need different approach obv\n",
    "\n",
    "tss_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fa9ce",
   "metadata": {},
   "source": [
    "# Scale Construction (EFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d70c5f",
   "metadata": {},
   "source": [
    "## Preliminary Data Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if items are appropriate for EFA\n",
    "# correlations mostly |.3|-|.8|\n",
    "items_corr = tss_df[item_labels_li].corr()\n",
    "\n",
    "extreme_corrs_count_dict = {}\n",
    "high_corr_li = []\n",
    "\n",
    "# print out extreme corrs\n",
    "total_report = \"\"\n",
    "for index, row in items_corr.iterrows():\n",
    "    variable1 = index\n",
    "    corrs = row\n",
    "    i = 0\n",
    "    count = 0\n",
    "    for variable2, corr in corrs.iteritems():\n",
    "        if (variable1 != variable2) and ((abs(corr) < 0.3) or (abs(corr) > 0.8)):\n",
    "            count += 1\n",
    "            if abs(corr) > 0.8:\n",
    "                high_corr_li.append((variable1, variable2, corr))\n",
    "        i += 1\n",
    "    extreme_corrs_count_dict[variable1] = count\n",
    "\n",
    "extreme_corrs_count = pd.Series(extreme_corrs_count_dict)\n",
    "print(\"Descriptives for extreme correlation count:\")\n",
    "print(extreme_corrs_count.describe())\n",
    "\n",
    "# uhmm that are more extreme correlations than I would like\n",
    "# but maybe subscales are fairly orthogonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c4d6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let us look at a heatmap (absolute values of correlations)\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "ax = sns.heatmap(abs(items_corr),\n",
    "                 xticklabels=items_corr.columns.values,\n",
    "                 yticklabels=items_corr.columns.values,\n",
    "                 ax=ax, cmap=\"viridis\", vmin=0.3, vmax=0.8)\n",
    "\n",
    "# most item clusters are just surprising orthogonal (which is actually nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be53e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect suspicious items with low correlations here\n",
    "susp_items = [\n",
    "    \"tss_cre_6\", \"tss_cre_10\", \"tss_cre_11\",\n",
    "    \"tss_rep_8\", \"tss_rep_10\",\n",
    "    \"tss_sty_8\", \"tss_sty_9\", \"tss_sty_12\"\n",
    "    \"tss_pac_1\", \"tss_pac_3\", \"tss_pac_5\", \"tss_pac_6\", \"tss_pac_9\"\n",
    "]\n",
    "\n",
    "for index, row in items_corr.iterrows():\n",
    "    if index in susp_items:\n",
    "        print(\"{}: {}\".format(index, items_descr[index]))\n",
    "        print(\"Correlations >= .3:\")\n",
    "        if row[(row >= .3) & (row.index != index)].empty:\n",
    "            print(\"NONE - ALL correlations are < .3!\")\n",
    "        else:\n",
    "            print(row[(row > .3) & (row.index != index)].to_string())\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b44e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider excluding variables with lots correlations < .3\n",
    "items_analysis_li = copy.deepcopy(item_labels_li)\n",
    "\n",
    "# excluded_items_li = [\n",
    "#     \"tss_sty_8\"\n",
    "# ]\n",
    "\n",
    "# for item in excluded_items_li:\n",
    "#     items_analysis_li.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f818e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity\n",
    "def reduce_multicoll(df, vars_li, vars_descr=[], print_details=True):\n",
    "    reduced_vars = copy.deepcopy(vars_li)\n",
    "    print(\"Beginning check for multicollinearity\")\n",
    "    vars_corr = df[reduced_vars].corr()\n",
    "    det = np.linalg.det(vars_corr)\n",
    "    print(\"\\nDeterminant of initial correlation matrix: {}\\n\".format(det))\n",
    "\n",
    "    if det > .00001:\n",
    "        print(\"Determinant is > .00001. No issues with multicollinearity detected.\")\n",
    "        return(reduced_vars)\n",
    "\n",
    "    print(\"Starting to remove redundant variables by acessing mutlicollinearity with VIF...\\n\")\n",
    "    count_missing = len(df) - len(df.dropna(subset=vars_li))\n",
    "    if count_missing > 0:\n",
    "        print(\"This requries dropping missing values.\",\n",
    "              \"The following procedure will ignore {} cases with missing values\".format(count_missing))\n",
    "    while det <= .00001:\n",
    "        # could implement pairwise dropping of missing here at some point\n",
    "        # but until you have a case with lots of missing data, this will work fine\n",
    "        x_df = df.dropna(subset=vars_li)[reduced_vars]\n",
    "        vifs = [vif(x_df.values, i)\n",
    "                for i in range(len(x_df.columns))]\n",
    "        vif_data = pd.Series(vifs, index=x_df.columns)\n",
    "        vif_max = (vif_data.idxmax(), vif_data.max())\n",
    "\n",
    "        if print_details:\n",
    "            print(\"Excluded item {}. VIF: {:.2f}\".format(\n",
    "                vif_max[0], vif_max[1]))\n",
    "\n",
    "            if len(vars_descr) > 0:\n",
    "                print(\"('{}')\".format(vars_descr[vif_max[0]]))\n",
    "            print(\"\")\n",
    "\n",
    "        reduced_vars.remove(vif_max[0])\n",
    "\n",
    "        vars_corr = df[reduced_vars].corr()\n",
    "        det = np.linalg.det(vars_corr)\n",
    "\n",
    "    print(\"Done! Determinant is now: {:.6f}\".format(det))\n",
    "    count_removed = len(vars_li) - len(reduced_vars)\n",
    "    print(\"I have excluded {} redunant items with {} items remaining\".format(\n",
    "        count_removed, len(reduced_vars)))\n",
    "\n",
    "    return(reduced_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913536ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "items_analysis_li = reduce_multicoll(\n",
    "    tss_df, items_analysis_li, vars_descr=items_descr, print_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b982c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check Sampling Adequacy (KMO)\n",
    "# (if SVD does not converge, dropna for participants with too many missing items)\n",
    "# (might need to experiment how many missing are still okay)\n",
    "\n",
    "tss_df.dropna(subset=items_analysis_li, thresh=1, inplace=True)\n",
    "\n",
    "kmo = fa.factor_analyzer.calculate_kmo(tss_df[items_analysis_li])\n",
    "\n",
    "print(\"Overall KMO: {}\".format(kmo[1]))\n",
    "\n",
    "i = 0\n",
    "low_item_kmo = False\n",
    "for item_kmo in kmo[0]:\n",
    "    if item_kmo < .6:\n",
    "        low_item_kmo = True\n",
    "        item_label = item_labels_li[i]\n",
    "        print(\"Low KMO for {} ('{}'): {}\".format(\n",
    "            item_label, items_descr[item_label], item_kmo))\n",
    "    i += 1\n",
    "\n",
    "if low_item_kmo == False:\n",
    "    print(\"All item KMOs are >.6\")\n",
    "\n",
    "# Guidelines for KMO (Kaiser & Rice, 1974)\n",
    "# Marvellous: values in the 0.90s\n",
    "# Meritorious: values in the 0.80s\n",
    "# Middling: values in the 0.70s\n",
    "# Mediocre: values in the 0.60s\n",
    "# Unacceptable: values in the 0.50s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3177a",
   "metadata": {},
   "source": [
    "## Running the EFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of factors\n",
    "# Use Screeplot & Parallel Analysis\n",
    "\n",
    "# Function to output results for parallel parallel analysis\n",
    "def parallel_analysis(df, vars_li, k=100, facs_to_display=15, print_graph=True, print_table=True):\n",
    "    # EFA with no rotation to get EVs\n",
    "    efa = fa.factor_analyzer.FactorAnalyzer(rotation=None)\n",
    "\n",
    "    # EVs for actual data\n",
    "    efa.fit(df[vars_li])\n",
    "    ev_pca, ev_efa = efa.get_eigenvalues()\n",
    "\n",
    "    # Prepare random data for parallel analysis\n",
    "    n, m = df[vars_li].shape\n",
    "    par_efa = fa.factor_analyzer.FactorAnalyzer(rotation=None)\n",
    "\n",
    "    # Create df to store the values\n",
    "    ev_par_df = pd.DataFrame(columns=range(1, m+1))\n",
    "\n",
    "    # Run the fit 'k' times over a random matrix\n",
    "    for runNum in range(0, k):\n",
    "        par_efa.fit(np.random.normal(size=(n, m)))\n",
    "        cur_ev_ser = pd.Series(par_efa.get_eigenvalues()[\n",
    "                               1], index=ev_par_df.columns)\n",
    "        ev_par_df = ev_par_df.append(cur_ev_ser, ignore_index=True)\n",
    "    # get 95th percentile for the evs\n",
    "    par_95per = ev_par_df.quantile(0.95)\n",
    "\n",
    "    if print_graph:\n",
    "        # Draw graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Line for eigenvalue 1\n",
    "        plt.plot([1, facs_to_display+1], [1, 1], 'k--', alpha=0.3)\n",
    "        # For the random data (parallel analysis)\n",
    "        plt.plot(range(1, len(par_95per[:facs_to_display])+1),\n",
    "                 par_95per[:facs_to_display], 'b', label='EVs - random', alpha=0.4)\n",
    "        # Markers and line for actual EFA eigenvalues\n",
    "        plt.scatter(\n",
    "            range(1, len(ev_efa[:facs_to_display])+1), ev_efa[:facs_to_display])\n",
    "        plt.plot(range(1, len(ev_efa[:facs_to_display])+1),\n",
    "                 ev_efa[:facs_to_display], label='EVs - survey data')\n",
    "\n",
    "        plt.title('Parallel Analysis Scree Plots', {'fontsize': 20})\n",
    "        plt.xlabel('Components', {'fontsize': 15})\n",
    "        plt.xticks(ticks=range(1, facs_to_display+1),\n",
    "                   labels=range(1, facs_to_display+1))\n",
    "        plt.ylabel('Eigenvalue', {'fontsize': 15})\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if print_table:\n",
    "        # Create simple table with values for 95th percentile for random data and EVs for actual data\n",
    "        print(\"Factor eigenvalues for the 95th percentile of {} random matricesand for survey data for first {} factors:\\n\".\n",
    "              format(k, facs_to_display))\n",
    "        print(\"\\033[1mFactor\\tEV - random data 95h perc.\\tEV survey data\\033[0m\")\n",
    "\n",
    "        last_index = 0\n",
    "        last_95per_par = 0\n",
    "        last_ev_efa = 0\n",
    "        found_threshold = False\n",
    "\n",
    "        # Loop that prints previous (!) values\n",
    "        # if current EV from survey data is smaller than 95th percentile from random data, we reached the threshold\n",
    "        # in that case print the previous values in bold as it marks the number of factors determined by parallel analysis\n",
    "        for index, cur_ev_par in par_95per[:facs_to_display].iteritems():\n",
    "            cur_ev_efa = ev_efa[index-1]\n",
    "            if (index > 1) & (cur_ev_par >= cur_ev_efa) & (found_threshold == False):\n",
    "                found_threshold = True\n",
    "                print(\"\\033[1m{}\\t{:.2f}\\t\\t\\t\\t{:.2f}\\033[0m\".format(\n",
    "                    last_index, last_95per_par, last_ev_efa))\n",
    "            elif (index > 1):\n",
    "                print(\"{}\\t{:.2f}\\t\\t\\t\\t{:.2f}\".format(\n",
    "                    last_index, last_95per_par, last_ev_efa))\n",
    "\n",
    "            if index == len(par_95per[:facs_to_display]):\n",
    "                print(\"{}\\t{:.2f}\\t\\t\\t\\t{:.2f}\".format(\n",
    "                    index, cur_ev_par, cur_ev_efa))\n",
    "\n",
    "            last_index = index\n",
    "            last_95per_par = cur_ev_par\n",
    "            last_ev_efa = cur_ev_efa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe388560",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_analysis(tss_df, items_analysis_li)\n",
    "\n",
    "# determine factors to investigate based on screeplot and parallel analysis\n",
    "# if both methods yield different results, examine both number of factors\n",
    "# (decide for one based on whether you get a clean solution and on how interpretable the solution is)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a6cd3",
   "metadata": {},
   "source": [
    "### 5 factor solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on rotation (orthogonal: Varimax, oblique: Oblimin)\n",
    "\n",
    "# Run oblique rotation\n",
    "efa = fa.FactorAnalyzer(n_factors=5, rotation='oblimin')\n",
    "efa.fit(tss_df[items_analysis_li])\n",
    "# Display factor correlation matrix\n",
    "print(\"\\nFactor Correlation Matrix (Oblique Rotation):\")\n",
    "print(efa.phi_)\n",
    "# If matrix has clear correlations between factors, than this indicates the need for an oblique rotation\n",
    "# if in doubt use oblique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_efa(data, vars_analsis, n_facs=4, rotation_method=\"Oblimin\",\n",
    "                 comm_thresh=0.2, main_thresh=0.4, cross_thres=0.3, load_diff_thresh=0.2,\n",
    "                 print_details=False, print_par_plot=False, print_par_table=False,\n",
    "                 par_k=100, par_n_facs=15, iterative=True):\n",
    "\n",
    "    efa = fa.FactorAnalyzer(n_factors=n_facs, rotation=rotation_method)\n",
    "\n",
    "    curr_vars = copy.deepcopy(vars_analsis)\n",
    "\n",
    "    efa.fit(data[curr_vars])\n",
    "\n",
    "    if print_par_plot or print_par_table:\n",
    "        parallel_analysis(data, curr_vars, k=par_k, facs_to_display=par_n_facs,\n",
    "                          print_graph=print_par_plot, print_table=print_par_table)\n",
    "\n",
    "    # Check 1: Check communcalities\n",
    "    print(\"Checking for low communalities\")\n",
    "    comms = pd.DataFrame(efa.get_communalities(),\n",
    "                         index=tss_df[curr_vars].columns, columns=['Communality'])\n",
    "    mask_low_comms = comms[\"Communality\"] < comm_thresh\n",
    "\n",
    "    if comms[mask_low_comms].empty:\n",
    "        print(\"All communalities above {}\\n\".format(comm_thresh))\n",
    "    else:\n",
    "        # save bad items and remove them\n",
    "        bad_items = comms[mask_low_comms].index\n",
    "        print(\"Detected {} items with low communality. Excluding them for next analysis.\\n\".format(\n",
    "            len(bad_items)))\n",
    "        for item in bad_items:\n",
    "            if print_details:\n",
    "                print(\"Removed item {}: {}\\nCommunality: {:.4f}\\n\".format(\n",
    "                    item, items_descr[item], comms.loc[item, \"Communality\"]))\n",
    "                curr_vars.remove(item)\n",
    "\n",
    "    # Check 2: Check for low main loading\n",
    "    print(\"Checking for low main loading\")\n",
    "    loadings = pd.DataFrame(efa.loadings_, index=data[curr_vars].columns)\n",
    "    max_loadings = abs(loadings).max(axis=1)\n",
    "    mask_low_main = max_loadings < main_thresh\n",
    "    if max_loadings[mask_low_main].empty:\n",
    "        print(\"All main loadings above {}\\n\".format(main_thresh))\n",
    "    else:\n",
    "        # save bad items and remove them\n",
    "        bad_items = max_loadings[mask_low_main].index\n",
    "        print(\"Detected {} items with low main loading. Excluding them for next analysis.\\n\".format(\n",
    "            len(bad_items)))\n",
    "        for item in bad_items:\n",
    "            if print_details:\n",
    "                print(\"Removed item {}: {}\\nMain (absolute) Loading: {:.4f}\\n\".format(\n",
    "                    item, items_descr[item], abs(loadings.loc[item]).max()))\n",
    "                curr_vars.remove(item)\n",
    "\n",
    "    # check 3: Check for high cross loadings\n",
    "    print(\"Checking high cross loadings\")\n",
    "\n",
    "    # create df that stores main_load, largest crossload and difference between the two\n",
    "    crossloads_df = pd.DataFrame(index=curr_vars)\n",
    "\n",
    "    crossloads_df[\"main_load\"] = abs(loadings).max(axis=1)\n",
    "    crossloads_df[\"cross_load\"] = abs(loadings).apply(\n",
    "        lambda row: row.nlargest(2).values[-1], axis=1)\n",
    "    crossloads_df[\"diff\"] = crossloads_df[\"main_load\"] - \\\n",
    "        crossloads_df[\"cross_load\"]\n",
    "\n",
    "    mask_high_cross = (crossloads_df[\"cross_load\"] > cross_thres) | (\n",
    "        crossloads_df[\"diff\"] < load_diff_thresh)\n",
    "\n",
    "    if crossloads_df[mask_high_cross].empty:\n",
    "        print(\"All cross_loadins loadings below {}\".format(cross_thres),\n",
    "              \"and differences between main loading and crossloadings above {}.\\n\".format(load_diff_thresh))\n",
    "    else:\n",
    "        # save bad items and remove them\n",
    "        bad_items = crossloads_df[mask_high_cross].index\n",
    "        print(\"Detected {} items with high cross loading. Excluding them for next analysis.\\n\"\n",
    "              .format(len(bad_items)))\n",
    "        for item in bad_items:\n",
    "            if print_details:\n",
    "                print(\"Removed item {}: {}\\nLoadings: \\n{}\\n\".format(\n",
    "                    item, items_descr[item], loadings.loc[item]))\n",
    "                curr_vars.remove(item)\n",
    "\n",
    "    corrs = data[curr_vars].corr()\n",
    "    det = np.linalg.det(corrs)\n",
    "    print(\"\\nDeterminant of correlation matrix: {}\".format(det))\n",
    "    if det > 0.00001:\n",
    "        print(\"Determinant looks good!\")\n",
    "    else:\n",
    "        print(\"Determinant is smaller than 0.00001!\")\n",
    "        print(\"Consider using stricer criteria and/or removing highly correlated vars\")\n",
    "\n",
    "    kmo = fa.factor_analyzer.calculate_kmo(data[curr_vars])\n",
    "    print(\"Overall KMO: {}\".format(kmo[1]))\n",
    "\n",
    "    i = 0\n",
    "    low_item_kmo = False\n",
    "    for item_kmo in kmo[0]:\n",
    "        if item_kmo < .6:\n",
    "            low_item_kmo = True\n",
    "            item_label = curr_vars[i]\n",
    "            print(\"Low KMO for {} ('{}'): {}\"\n",
    "                  .format(item_label, items_descr[item_label], item_kmo))\n",
    "        i += 1\n",
    "    if low_item_kmo == False:\n",
    "        print(\"All item KMOs are >.6\")\n",
    "\n",
    "    return(efa, curr_vars)\n",
    "\n",
    "\n",
    "def iterative_efa(data, vars_analsis, n_facs=4, rotation_method=\"Oblimin\",\n",
    "                  comm_thresh=0.2, main_thresh=0.4, cross_thres=0.3, load_diff_thresh=0.2,\n",
    "                  print_details=False, print_par_plot=False, print_par_table=False,\n",
    "                  par_k=100, par_n_facs=15, iterative=True):\n",
    "    efa = fa.FactorAnalyzer(n_factors=n_facs, rotation=rotation_method)\n",
    "\n",
    "    final_solution = False\n",
    "\n",
    "    curr_vars = copy.deepcopy(vars_analsis)\n",
    "\n",
    "    i = 1\n",
    "    while final_solution == False:\n",
    "        # Fit EFA\n",
    "        efa.fit(data[curr_vars])\n",
    "        print(\"Fitted solution #{}\\n\".format(i))\n",
    "\n",
    "        # print screeplot and/or table for parallel analysis if option was chosen\n",
    "        # but not for first run (assuming it was already checked to select number of factors)\n",
    "        if (i > 1) and print_par_plot or print_par_table:\n",
    "            parallel_analysis(data, curr_vars, k=par_k, facs_to_display=par_n_facs,\n",
    "                              print_graph=print_par_plot, print_table=print_par_table)\n",
    "\n",
    "        # Check 1: Check communcalities\n",
    "        print(\"Checking for low communalities\")\n",
    "        comms = pd.DataFrame(efa.get_communalities(\n",
    "        ), index=tss_df[curr_vars].columns, columns=['Communality'])\n",
    "        mask_low_comms = comms[\"Communality\"] < comm_thresh\n",
    "\n",
    "        if comms[mask_low_comms].empty:\n",
    "            print(\"All communalities above {}\\n\".format(comm_thresh))\n",
    "        else:\n",
    "            # save bad items and remove them\n",
    "            bad_items = comms[mask_low_comms].index\n",
    "            print(\"Detected {} items with low communality. Excluding them for next analysis.\\n\".format(\n",
    "                len(bad_items)))\n",
    "            for item in bad_items:\n",
    "                if print_details:\n",
    "                    print(\"Removed item {}: {}\\nCommunality: {:.4f}\\n\".format(\n",
    "                        item, items_descr[item], comms.loc[item, \"Communality\"]))\n",
    "                curr_vars.remove(item)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Check 2: Check for low main loading\n",
    "        print(\"Checking for low main loading\")\n",
    "        loadings = pd.DataFrame(efa.loadings_, index=data[curr_vars].columns)\n",
    "        max_loadings = abs(loadings).max(axis=1)\n",
    "        mask_low_main = max_loadings < main_thresh\n",
    "        if max_loadings[mask_low_main].empty:\n",
    "            print(\"All main loadings above {}\\n\".format(main_thresh))\n",
    "        else:\n",
    "            # save bad items and remove them\n",
    "            bad_items = max_loadings[mask_low_main].index\n",
    "            print(\"Detected {} items with low main loading. Excluding them for next analysis.\\n\".format(\n",
    "                len(bad_items)))\n",
    "            for item in bad_items:\n",
    "                if print_details:\n",
    "                    print(\"Removed item {}: {}\\nMain (absolute) Loading: {:.4f}\\n\".format(\n",
    "                        item, items_descr[item], abs(loadings.loc[item]).max()))\n",
    "                curr_vars.remove(item)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # check 3: Check for high cross loadings\n",
    "        print(\"Checking high cross loadings\")\n",
    "\n",
    "        # create df that stores main_load, largest crossload and difference between the two\n",
    "        crossloads_df = pd.DataFrame(index=curr_vars)\n",
    "\n",
    "        crossloads_df[\"main_load\"] = abs(loadings).max(axis=1)\n",
    "        crossloads_df[\"cross_load\"] = abs(loadings).apply(\n",
    "            lambda row: row.nlargest(2).values[-1], axis=1)\n",
    "        crossloads_df[\"diff\"] = crossloads_df[\"main_load\"] - \\\n",
    "            crossloads_df[\"cross_load\"]\n",
    "\n",
    "        mask_high_cross = (crossloads_df[\"cross_load\"] > cross_thres) | (\n",
    "            crossloads_df[\"diff\"] < load_diff_thresh)\n",
    "\n",
    "        if crossloads_df[mask_high_cross].empty:\n",
    "            print(\"All cross_loadins loadings below {}\".format(cross_thres),\n",
    "                  \"and differences between main loading and crossloadings above {}.\\n\".format(load_diff_thresh))\n",
    "        else:\n",
    "            # save bad items and remove them\n",
    "            bad_items = crossloads_df[mask_high_cross].index\n",
    "            print(\"Detected {} items with high cross loading. Excluding them for next analysis.\\n\".format(\n",
    "                len(bad_items)))\n",
    "            for item in bad_items:\n",
    "                if print_details:\n",
    "                    print(\"Removed item {}: {}\\nLoadings: \\n{}\\n\".format(\n",
    "                        item, items_descr[item], loadings.loc[item]))\n",
    "                curr_vars.remove(item)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        print(\"Final solution reached.\")\n",
    "        final_solution = True\n",
    "\n",
    "        corrs = data[curr_vars].corr()\n",
    "        det = np.linalg.det(corrs)\n",
    "        print(\"\\nDeterminant of correlation matrix: {}\".format(det))\n",
    "        if det > 0.00001:\n",
    "            print(\"Determinant looks good!\")\n",
    "        else:\n",
    "            print(\"Determinant is smaller than 0.00001!\")\n",
    "            print(\n",
    "                \"Consider using stricer criteria and/or removing highly correlated vars\")\n",
    "\n",
    "        kmo = fa.factor_analyzer.calculate_kmo(data[curr_vars])\n",
    "        print(\"Overall KMO: {}\".format(kmo[1]))\n",
    "\n",
    "        i = 0\n",
    "        low_item_kmo = False\n",
    "        for item_kmo in kmo[0]:\n",
    "            if item_kmo < .6:\n",
    "                low_item_kmo = True\n",
    "                item_label = curr_vars[i]\n",
    "                print(\"Low KMO for {} ('{}'): {}\".format(\n",
    "                    item_label, items_descr[item_label], item_kmo))\n",
    "            i += 1\n",
    "        if low_item_kmo == False:\n",
    "            print(\"All item KMOs are >.6\")\n",
    "\n",
    "# Determinant should be > .00001\n",
    "\n",
    "    return(efa, curr_vars)\n",
    "\n",
    "# print relevant loadings for each factor to output\n",
    "\n",
    "\n",
    "def print_sorted_loadings(efa, item_lables, load_thresh=0.4, descr=[]):\n",
    "    loadings = pd.DataFrame(efa.loadings_, index=item_lables)\n",
    "    n_load = loadings.shape[1]\n",
    "\n",
    "    if len(descr) > 0:\n",
    "        loadings[\"descr\"] = loadings.apply(lambda x: descr[x.name], axis=1)\n",
    "\n",
    "    for i in range(0, n_load):\n",
    "        mask_relev_loads = abs(loadings[i]) > load_thresh\n",
    "        sorted_loads = loadings[mask_relev_loads].sort_values(\n",
    "            i, key=abs, ascending=False)\n",
    "        print(\"Relevant loadings for factor {}\".format(i))\n",
    "        if len(descr) > 0:\n",
    "            print(sorted_loads[[i, \"descr\"]].to_string(), \"\\n\")\n",
    "        else:\n",
    "            print(sorted_loads[i].to_string(), \"\\n\")\n",
    "\n",
    "\n",
    "def rev_items_and_return(df, efa, item_lables, load_thresh=0.4, min_score=1, max_score=5):\n",
    "\n",
    "    loadings = pd.DataFrame(efa.loadings_, index=item_lables)\n",
    "    n_load = loadings.shape[1]\n",
    "\n",
    "    items_per_fact_dict = {}\n",
    "\n",
    "    # loop through n factors\n",
    "    # determine relevant items that are positive (can just be used as is)\n",
    "    # and items with negative loads (need to be refersed)\n",
    "    for i in range(0, n_load):\n",
    "        mask_pos_loads = loadings[i] > load_thresh\n",
    "        mask_neg_loads = loadings[i] < -load_thresh\n",
    "        pos_items = loadings[mask_pos_loads].index.tolist()\n",
    "        neg_items = loadings[mask_neg_loads].index.tolist()\n",
    "\n",
    "        # add items with positive items directly to dict\n",
    "        items_per_fact_dict[i] = pos_items\n",
    "\n",
    "        # create reverse-coded item in df for items with negative loadings\n",
    "        for item in neg_items:\n",
    "            rev_item_name = item + \"_rev\"\n",
    "            df[rev_item_name] = (df[item] - (max_score+min_score)) * -1\n",
    "            items_per_fact_dict[i].append(rev_item_name)\n",
    "\n",
    "    return items_per_fact_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82281a99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "five_facs = iterative_efa(tss_df, items_analysis_li, n_facs=5, rotation_method=\"Oblimin\", print_details=True,\n",
    "                          print_par_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc94847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "efa_5 = five_facs[0]\n",
    "items_5 = five_facs[1]\n",
    "\n",
    "print_sorted_loadings(efa_5, items_5, load_thresh=0.4, descr=items_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd366e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items_per_fac_5 = rev_items_and_return(tss_df, efa_5, items_5)\n",
    "\n",
    "for factor_n in items_per_fac_5:\n",
    "    print(\"Internal consistency for factor {}:\".format(factor_n))\n",
    "\n",
    "    items = items_per_fac_5[factor_n]\n",
    "\n",
    "    if len(items) > 2:\n",
    "        cron_alpha = pg.cronbach_alpha(data=tss_df[items], ci=.95)\n",
    "        print(\"Cronbachs alpha = {:.4f}, 95% CI = [{:.2f}, {:.2f}]\".format(\n",
    "            cron_alpha[0], cron_alpha[1][0], cron_alpha[1][1]))\n",
    "\n",
    "        # loop over items for current factor\n",
    "        # compute cronbach's alpha by excluding one item at a time\n",
    "        print(\"\\nCronbach's alpha when excluding variable...\")\n",
    "        for cur_item in items:\n",
    "            # create list with all items except current item\n",
    "            items_wo_cur_item = copy.deepcopy(items)\n",
    "            items_wo_cur_item.remove(cur_item)\n",
    "\n",
    "            cur_cron_alpha = pg.cronbach_alpha(\n",
    "                data=tss_df[items_wo_cur_item], ci=.95)[0]\n",
    "\n",
    "            # bold entry if excluding item leads to improvement\n",
    "            bold_or_not = \"\\033[1m\" if cur_cron_alpha > cron_alpha[0] else \"\\033[0m\"\n",
    "            print(\"{}{}: {:.4f}\\033[0m\".format\n",
    "                  (bold_or_not, cur_item, cur_cron_alpha))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "# consider dropping items at this point\n",
    "# Consider dropping alpha if it would noticeable improve internal consistency\n",
    "# Drop items if you have > 6 items and dropping rest would not noticeable decrease internal consistency\n",
    "# (also take loadings into account here)\n",
    "# Be wary of dropping items if it would bring you down to 2 items..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30afc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_5_2 = copy.deepcopy(items_5)\n",
    "\n",
    "items_5_2.remove(\"tss_rep_11\")\n",
    "\n",
    "# items_5_2.extend([\"tss_rep_4\", \"tss_rep_5\",\n",
    "#                   \"tss_rep_6\", \"tss_rep_7\", \"tss_rep_10\", \"tss_rep_12\"])\n",
    "# items_5_2.extend([\"tss_cre_1\", \"tss_cre_2\", \"tss_cre_4\",\n",
    "#                  \"tss_cre_5\", \"tss_cre_8\", \"tss_cre_10\", \"tss_cre_11\"])\n",
    "# items_5_2.extend([\"tss_pac_2\", \"tss_pac_4\"])\n",
    "#items_5_2.extend([\"tss_sty_1\", \"tss_sty_2\", \"tss_sty_4\", \"tss_sty_11\"])\n",
    "items_5_2.extend([\"tss_coh_1\", \"tss_coh_2\", \"tss_coh_4\", \"tss_coh_5\",\n",
    "                  \"tss_coh_7\", \"tss_coh_8\", \"tss_coh_10\", \"tss_coh_12\",\n",
    "                  \"tss_conch_1\", \"tss_conch_3\", \n",
    "                  \"tss_conch_5\", \"tss_conch_6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a6769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items_5_2 = reduce_multicoll(\n",
    "    tss_df, items_5_2, vars_descr=items_descr, print_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71cba7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "five_facs_2 = iterative_efa(tss_df, items_5_2, n_facs=5, rotation_method=\"Oblimin\", print_details=True,\n",
    "                            print_par_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_5_2 = five_facs_2[0]\n",
    "items_5_2 = five_facs_2[1]\n",
    "\n",
    "print_sorted_loadings(efa_5_2, items_5_2, load_thresh=0.4, descr=items_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_per_fac_5_2 = rev_items_and_return(tss_df, efa_5_2, items_5_2)\n",
    "\n",
    "for factor_n in items_per_fac_5_2:\n",
    "    print(\"Internal consistency for factor {}:\".format(factor_n))\n",
    "\n",
    "    items = items_per_fac_5_2[factor_n]\n",
    "\n",
    "    if len(items) > 2:\n",
    "        cron_alpha = pg.cronbach_alpha(data=tss_df[items], ci=.95)\n",
    "        print(\"Cronbachs alpha = {:.4f}, 95% CI = [{:.2f}, {:.2f}]\".format(\n",
    "            cron_alpha[0], cron_alpha[1][0], cron_alpha[1][1]))\n",
    "\n",
    "        # loop over items for current factor\n",
    "        # compute cronbach's alpha by excluding one item at a time\n",
    "        print(\"\\nCronbach's alpha when excluding variable...\")\n",
    "        for cur_item in items:\n",
    "            # create list with all items except current item\n",
    "            items_wo_cur_item = copy.deepcopy(items)\n",
    "            items_wo_cur_item.remove(cur_item)\n",
    "\n",
    "            cur_cron_alpha = pg.cronbach_alpha(\n",
    "                data=tss_df[items_wo_cur_item], ci=.95)[0]\n",
    "\n",
    "            # bold entry if excluding item leads to improvement\n",
    "            bold_or_not = \"\\033[1m\" if cur_cron_alpha > cron_alpha[0] else \"\\033[0m\"\n",
    "            print(\"{}{}: {:.4f}\\033[0m\".format\n",
    "                  (bold_or_not, cur_item, cur_cron_alpha))\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_per_fac_4_2[1].remove(\"tss_rep_11\")\n",
    "# items_per_fac_4_2[2].remove(\"tss_cre_9\")\n",
    "items_per_fac_5_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d4501",
   "metadata": {},
   "source": [
    "# Examine Scale Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06128e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute scale means but only if at least 66% of questions are answered\n",
    "mask_suff_data_coh = tss_df[items_per_fac_5_2[0]].isna().sum(\n",
    "    axis=1) <= round(len(items_per_fac_5_2[0])/3)\n",
    "mask_suff_data_cre = tss_df[items_per_fac_5_2[1]].isna().sum(\n",
    "    axis=1) <= round(len(items_per_fac_5_2[1])/3)\n",
    "mask_suff_data_pac = tss_df[items_per_fac_5_2[2]].isna().sum(\n",
    "    axis=1) <= round(len(items_per_fac_5_2[2])/3)\n",
    "mask_suff_data_conch = tss_df[items_per_fac_5_2[3]].isna().sum(\n",
    "    axis=1) <= round(len(items_per_fac_5_2[3])/3)\n",
    "mask_suff_data_rep = tss_df[items_per_fac_5_2[4]].isna().sum(\n",
    "    axis=1) <= round(len(items_per_fac_5_2[4])/3)\n",
    "\n",
    "tss_df[\"tss_coh\"] = tss_df[mask_suff_data_coh][items_per_fac_5_2[0]].mean(\n",
    "    axis=1)\n",
    "tss_df[\"tss_cre\"] = tss_df[mask_suff_data_cre][items_per_fac_5_2[1]].mean(\n",
    "    axis=1)\n",
    "tss_df[\"tss_pac\"] = tss_df[mask_suff_data_pac][items_per_fac_5_2[2]].mean(\n",
    "    axis=1)\n",
    "tss_df[\"tss_conch\"] = tss_df[mask_suff_data_cre][items_per_fac_5_2[3]].mean(\n",
    "    axis=1)\n",
    "tss_df[\"tss_rep\"] = tss_df[mask_suff_data_rep][items_per_fac_5_2[4]].mean(\n",
    "    axis=1)\n",
    "\n",
    "# Some scales might make more sense fully reversed\n",
    "tss_df[\"tss_pac\"] = (tss_df[\"tss_pac\"]-6)*-1\n",
    "tss_df[\"tss_conch\"] = (tss_df[\"tss_conch\"]-6)*-1\n",
    "tss_df[\"tss_avoid_rep\"] = (tss_df[\"tss_rep\"]-6)*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you actually have missing data, double check if this worked as intended\n",
    "tss_df[~mask_suff_data_cre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80472b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scale distributions\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 15))\n",
    "\n",
    "# Histograms with KDE\n",
    "ax1 = tss_df[\"tss_coh\"].plot.hist(ax=axes[0, 0], bins=12)\n",
    "ax1.set_xlabel(\"Story Cohesion\")\n",
    "ax1.set_xlim(1, 5)\n",
    "tss_df[\"tss_coh\"].plot.kde(ax=axes[0, 0], secondary_y=True)\n",
    "\n",
    "ax2 = tss_df[\"tss_cre\"].plot.hist(ax=axes[1, 0], bins=12)\n",
    "ax2.set_xlabel(\"Story Creativity\")\n",
    "ax2.set_xlim(1, 5)\n",
    "tss_df[\"tss_cre\"].plot.kde(ax=axes[1, 0], secondary_y=True)\n",
    "\n",
    "ax3 = tss_df[\"tss_pac\"].plot.hist(ax=axes[2, 0], bins=12)\n",
    "tss_df[\"tss_pac\"].plot.kde(ax=axes[2, 0], secondary_y=True)\n",
    "ax3.set_xlabel(\"Story Pace\")\n",
    "ax3.set_xlim(1, 5)\n",
    "\n",
    "ax4 = tss_df[\"tss_avoid_rep\"].plot.hist(ax=axes[3, 0], bins=12)\n",
    "tss_df[\"tss_avoid_rep\"].plot.kde(ax=axes[3, 0], secondary_y=True)\n",
    "ax4.set_xlabel(\"Avoiding Repetitiveness\")\n",
    "ax4.set_xlim(1, 5)\n",
    "\n",
    "ax5 = tss_df[\"tss_conch\"].plot.hist(ax=axes[4, 0], bins=12)\n",
    "tss_df[\"tss_conch\"].plot.kde(ax=axes[4, 0], secondary_y=True)\n",
    "ax5.set_xlabel(\"Consistent Characters\")\n",
    "ax5.set_xlim(1, 5)\n",
    "\n",
    "# Q-Q Plots\n",
    "stats.probplot(tss_df[\"tss_coh\"], dist=\"norm\", plot=axes[0, 1])\n",
    "stats.probplot(tss_df[\"tss_cre\"], dist=\"norm\", plot=axes[1, 1])\n",
    "stats.probplot(tss_df[\"tss_pac\"], dist=\"norm\", plot=axes[2, 1])\n",
    "stats.probplot(tss_df[\"tss_avoid_rep\"], dist=\"norm\", plot=axes[3, 1])\n",
    "stats.probplot(tss_df[\"tss_conch\"], dist=\"norm\", plot=axes[4, 1])\n",
    "\n",
    "# Boxplots\n",
    "tss_df[\"tss_coh\"].plot.box(ax=axes[0, 2])\n",
    "tss_df[\"tss_cre\"].plot.box(ax=axes[1, 2])\n",
    "tss_df[\"tss_pac\"].plot.box(ax=axes[2, 2])\n",
    "tss_df[\"tss_avoid_rep\"].plot.box(ax=axes[3, 2])\n",
    "tss_df[\"tss_conch\"].plot.box(ax=axes[4, 2])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_df[[\"tss_coh\", \"tss_cre\", \"tss_avoid_rep\", \"tss_pac\", \"tss_conch\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269212fa",
   "metadata": {},
   "source": [
    "# Analyze Impact of Preset & Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "tidy = tss_df[[\"response_id\", \"prompt_label\",\n",
    "               \"tss_coh\", \"tss_cre\", \"tss_avoid_rep\", \"tss_pac\", \"tss_conch\"]\n",
    "              ].melt(id_vars=['response_id', \"prompt_label\"])\n",
    "\n",
    "ax = sns.catplot(data=tidy,\n",
    "                 kind=\"bar\",\n",
    "                 y=\"value\", x=\"variable\", hue=\"prompt_label\",\n",
    "                 ci=90, height=5, aspect=2)\n",
    "\n",
    "plt.ylim(1, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "tidy = tss_df[[\"response_id\", \"preset_label\",\n",
    "               \"tss_coh\", \"tss_cre\", \"tss_avoid_rep\", \"tss_pac\", \"tss_conch\"]\n",
    "              ].melt(id_vars=['response_id', \"preset_label\"])\n",
    "\n",
    "ax = sns.catplot(data=tidy,\n",
    "                 kind=\"bar\",\n",
    "                 y=\"value\", x=\"variable\", hue=\"preset_label\",\n",
    "                 ci=90, height=5, aspect=2)\n",
    "\n",
    "plt.ylim(1, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cba78",
   "metadata": {},
   "source": [
    "## Use Word Count as a Control Variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "fig.tight_layout(pad=5.0)\n",
    "plt.grid(False)\n",
    "\n",
    "sns.regplot(data=tss_df, y=\"tss_coh\",\n",
    "            x=\"word_count\", lowess=True, ax=axes[0])\n",
    "sns.regplot(data=tss_df, y=\"tss_avoid_rep\",\n",
    "            x=\"word_count\", lowess=True, ax=axes[1])\n",
    "sns.regplot(data=tss_df, y=\"tss_pac\",\n",
    "            x=\"word_count\", lowess=True, ax=axes[2])\n",
    "sns.regplot(data=tss_df, y=\"tss_cre\",\n",
    "            x=\"word_count\", lowess=True, ax=axes[3])\n",
    "sns.regplot(data=tss_df, y=\"tss_conch\",\n",
    "            x=\"word_count\", lowess=True, ax=axes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39875f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up k-fold cross validation\n",
    "\n",
    "kf = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining predictors for models\n",
    "tss_df[\"words_cent\"] = tss_df[\"word_count\"].apply(\n",
    "    lambda x: x-tss_df[\"word_count\"].mean())  # centering\n",
    "\n",
    "tss_df[\"words_cent**2\"] = tss_df[\"words_cent\"]*tss_df[\"words_cent\"]\n",
    "tss_df[\"words_cent**3\"] = tss_df[\"words_cent\"]**3\n",
    "#tss_df[\"words_cent**4\"] = tss_df[\"words_cent\"]**4\n",
    "#tss_df[\"words_cent**5\"] = tss_df[\"words_cent\"]**5\n",
    "\n",
    "lin_model = linreg()\n",
    "\n",
    "pred_li = [(\"linear\", \"words_cent\"), (\"quadratic\", \"words_cent**2\"),\n",
    "           (\"cubic\", \"words_cent**3\")]\n",
    "#(\"quartic\", \"words_cent**4\"),\n",
    "# (\"quintic\", \"words_cent**5\")]\n",
    "outcome_li = [\"tss_coh\", \"tss_cre\", \"tss_avoid_rep\", \"tss_pac\", \"tss_conch\"]\n",
    "\n",
    "\n",
    "for outcome in outcome_li:\n",
    "    y = tss_df[outcome]\n",
    "    current_preds_col = []\n",
    "\n",
    "    for pred in pred_li:\n",
    "        current_preds_col.append(pred[1])\n",
    "        x = tss_df.loc[:, current_preds_col]\n",
    "\n",
    "        current_model_name = pred[0]\n",
    "\n",
    "        cross_val = cross_validate(lin_model, x, y,\n",
    "                                   scoring=[\"neg_mean_squared_error\", \"r2\"], cv=kf)\n",
    "\n",
    "        neg_mses = cross_val[\"test_neg_mean_squared_error\"]\n",
    "        r_squares = cross_val[\"test_r2\"]\n",
    "        avg_rmse = np.mean((neg_mses*-1)**0.5)\n",
    "        avg_r_sq = np.mean(r_squares)\n",
    "        print(\"Model performance for {} model predicting {}:\".format(\n",
    "            current_model_name, outcome))\n",
    "        print(\"r-square: {:.4f}    RMSE: {:.4f}\".format(avg_r_sq, avg_rmse))\n",
    "    print(\"\")\n",
    "\n",
    "# word count is completly irrelevant to story ratings...\n",
    "# if it stays this way I could just run a MANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e1504",
   "metadata": {},
   "source": [
    "## ANOVA Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a72b43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "sns.boxplot(x=\"preset_label\", y=\"tss_coh\", data=tss_df, ax=axes[0])\n",
    "sns.boxplot(x=\"preset_label\", y=\"tss_avoid_rep\", data=tss_df, ax=axes[1])\n",
    "sns.boxplot(x=\"preset_label\", y=\"tss_pac\", data=tss_df, ax=axes[2])\n",
    "sns.boxplot(x=\"preset_label\", y=\"tss_cre\", data=tss_df, ax=axes[3])\n",
    "sns.boxplot(x=\"preset_label\", y=\"tss_conch\", data=tss_df, ax=axes[4])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "fig.tight_layout(pad=5.0)\n",
    "plt.grid(False)\n",
    "\n",
    "sns.boxplot(x=\"prompt_label\", y=\"tss_coh\", data=tss_df, ax=axes[0])\n",
    "sns.boxplot(x=\"prompt_label\", y=\"tss_avoid_rep\", data=tss_df, ax=axes[1])\n",
    "sns.boxplot(x=\"prompt_label\", y=\"tss_pac\", data=tss_df, ax=axes[2])\n",
    "sns.boxplot(x=\"prompt_label\", y=\"tss_cre\", data=tss_df, ax=axes[3])\n",
    "sns.boxplot(x=\"prompt_label\", y=\"tss_conch\", data=tss_df, ax=axes[4])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa0b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def determine_outliers(df, var, distance=3, mode=\"print\"):\n",
    "    q1 = df[var].quantile(0.25)\n",
    "    q3 = df[var].quantile(0.75)\n",
    "\n",
    "    iqr = q3-q1\n",
    "    outlier_lower = q1 - (iqr*distance)\n",
    "    outlier_upper = q3 + (iqr*distance)\n",
    "    if mode == \"print\":\n",
    "        print(\"25th Percentile (Q1): {:.2f}\\n75th Percentile (Q3): {:.2f}\\nIQR: {:.2f}\".format(\n",
    "            q1, q3, iqr))\n",
    "        print(\"Will count cases as outlier with values less than {:.2f} or more than {:.2f}.\"\n",
    "              .format(outlier_lower, outlier_upper))\n",
    "        mask_outlier = (df[var] < outlier_lower) | (df[var] > outlier_upper)\n",
    "        if df[mask_outlier].shape[0] == 0:\n",
    "            print(\"With these criteria there are no outlier in the data\")\n",
    "        else:\n",
    "            print(\"Showing outliers\")\n",
    "            print(df[mask_outlier][var])\n",
    "    else:\n",
    "        print(\"Mode must be 'print'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302212ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for Presets\n",
    "\n",
    "# Outliers Coherence\n",
    "# print(\"Exterme outliers on tss_coh, Basic Coherence preset\")\n",
    "# determine_outliers(tss_df[tss_df[\"preset_label\"] == \"Basic Coherence\"], \"tss_coh\")\n",
    "\n",
    "# Outliers Repitition\n",
    "print(\"\\nExterme outliers on tss_rep, Morpho preset\")\n",
    "determine_outliers(tss_df[tss_df[\"preset_label\"] == \"Morpho\"], \"tss_avoid_rep\")\n",
    "print(\"\\nExterme outliers on tss_rep, Fandango preset\")\n",
    "determine_outliers(tss_df[tss_df[\"preset_label\"] == \"Fandango\"], \"tss_avoid_rep\")\n",
    "\n",
    "# # Outliers pace\n",
    "# print(\"\\nExterme outliers on tss_pac, Morpho preset\")\n",
    "# determine_outliers(tss_df[tss_df[\"preset_label\"] == \"Morpho\"], \"tss_pac\")\n",
    "\n",
    "# Outliers creativity\n",
    "print(\"\\nExterme outliers on tss_cre, Low Rider preset\")\n",
    "determine_outliers(tss_df[tss_df[\"preset_label\"] == \"Low Rider\"], \"tss_cre\")\n",
    "print(\"\\nExterme outliers on tss_pac, Ouroboros preset\")\n",
    "determine_outliers(tss_df[tss_df[\"preset_label\"] == \"Ouroboros\"], \"tss_cre\")\n",
    "print(\"\\nExterme outliers on tss_pac, Ouroboros preset\")\n",
    "determine_outliers(tss_df[tss_df[\"preset_label\"] == \"Basic Coherence\"], \"tss_cre\")\n",
    "\n",
    "# Outliers for Prompts\n",
    "\n",
    "# # Coherence\n",
    "# print(\"\\nExterme outliers on tss_coh, Hard Sci-Fi prompt\")\n",
    "# determine_outliers(tss_df[tss_df[\"prompt_label\"] == \"Hard Sci-Fi\"], \"tss_coh\")\n",
    "\n",
    "# Pace\n",
    "print(\"\\nExterme outliers on tss_pac, Horror prompt\")\n",
    "determine_outliers(tss_df[tss_df[\"prompt_label\"] == \"Horror\"], \"tss_pac\")\n",
    "\n",
    "# Creativity\n",
    "print(\"\\nExterme outliers on tss_cre, Romance prompt\")\n",
    "determine_outliers(tss_df[tss_df[\"prompt_label\"] == \"Historical Romance\"], \"tss_cre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tss_no_out = tss_df[~(tss_df.index == 7)].copy()\n",
    "tss_no_out = tss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b8df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normality\n",
    "def qqs_over_groups_and_vars(df, group_label, vars_li, size=(15, 15)):\n",
    "    groups_li = df[group_label].unique()\n",
    "    fig, axes = plt.subplots(len(groups_li), len(vars_li), figsize=size)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "    plt.grid(False)\n",
    "\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for group, var in itertools.product(groups_li, vars_li):\n",
    "        stats.probplot(df[df[group_label] == group][var],\n",
    "                       dist=\"norm\", plot=axes[y, x])\n",
    "        axes[y, x].set_title(group + \" - \" + var)\n",
    "        if x < (len(vars_li)-1):\n",
    "            x += 1\n",
    "        else:\n",
    "            x = 0\n",
    "            y += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqs_over_groups_and_vars(tss_no_out, \"preset_label\", outcome_li, size=(20, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqs_over_groups_and_vars(tss_no_out, \"prompt_label\", outcome_li, size=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4d389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if F-test ist robust to heteroscedacity\n",
    "# Taking recommended approach from Blanca et al., 2018\n",
    "def anova_check_homoscedacity(y_var, group_var, df):\n",
    "    var_ser = pd.Series(index=df[group_var].unique(), dtype=float)\n",
    "\n",
    "    for group in df[group_var].unique():\n",
    "        var_ser[group] = df[df[group_var] == group][y_var].var()\n",
    "\n",
    "    min_var = (var_ser.idxmin(), var_ser.min())\n",
    "    max_var = (var_ser.idxmax(), var_ser.max())\n",
    "    var_ratio = max_var[1]/min_var[1]\n",
    "    print(\"Smallest variance for {}: {:.2f}\".format(min_var[0], min_var[1]))\n",
    "    print(\"Largest variance for {}: {:.2f}\".format(max_var[0], max_var[1]))\n",
    "    print(\"Variance ratio for: {:.2f}\".format(var_ratio))\n",
    "\n",
    "    if var_ratio <= 1.5:\n",
    "        print(\"Variance ratio is smaller or equal to 1.5, F-test will be robust.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Variance ratio is larger 1.5. Now doing additional checks to see if F-test is robust.\")\n",
    "\n",
    "    # Create dataframe with variance and group sizes\n",
    "    var_n_df = var_ser.to_frame(name=\"var\")\n",
    "    var_n_df[\"n\"] = df.value_counts(subset=group_var)\n",
    "    # get correlation between correlation and variance\n",
    "    corr_var_n = var_n_df[[\"var\", \"n\"]].corr().iloc[1, 0]\n",
    "\n",
    "    if (corr_var_n >= 0) and (corr_var_n <= 0.5):\n",
    "        print(\"Correlation between sample size and variance (pairing) is {:.2f}. That is between 0 and .5. F-test should be robust\".\n",
    "              format(corr_var_n))\n",
    "        return\n",
    "    else:\n",
    "        print(\"Correlation between sample size and variance (pairing) is {:.2f}. That is below 0 or over .5.\".\n",
    "              format(corr_var_n), \"Continuing with further checks...\")\n",
    "\n",
    "    # Compute coefficient of sample size variation\n",
    "    coeff_n = var_n_df[\"var\"].std()/var_n_df[\"var\"].mean()\n",
    "    if (corr_var_n > 0.5) and (coeff_n > .33) and (var_ratio > 2):\n",
    "        print(\"Pairing is {:.2f}, so larger than .5,\".format(corr_var_n),\n",
    "              \",coefficient of sample size variation is {:.2f}, larger than .33,\".format(\n",
    "                  coeff_n),\n",
    "              \"and variance ratio is {:.2f}, larger than 2.\".format(var_ratio),\n",
    "              \"F-test is too conserative (hurting power)\")\n",
    "    elif (corr_var_n < 0) and (corr_var_n >= -0.5) and (coeff_n > .16) and (var_ratio > 2):\n",
    "        print(\"Pairing is {:.2f}, so smaller than 0 and larger than or equal to -.5,\".format(corr_var_n),\n",
    "              \",coefficient of sample size variation is {:.2f}, larger than .16,\".format(\n",
    "                  coeff_n),\n",
    "              \"and variance ratio is {:.2f}, larger than 2.\".format(var_ratio),\n",
    "              \"F-test is too liberal (real alpha might be as high as .1 if variance ratio is 9 or smaller).\")\n",
    "    elif (corr_var_n < -0.5):\n",
    "        print(\"Pairing is {:.2f}, so smaller than -.5.\".format(corr_var_n),\n",
    "              \"F-test is too liberal (real alpha might be as high as .2 if variance ratio is 9 or smaller).\")\n",
    "    else:\n",
    "        print(\"Pairing is {:.2f}, coefficient of sample size variation is {:.2f}, variance ratio is {:.2f}.\"\n",
    "              .format(corr_var_n, coeff_n, var_ratio),\n",
    "              \"This specific combination should have robust F-test, but look into the paper\",\n",
    "              \"('Effect of variance ratio on ANOVA robustness: Might 1.5 be the limit?', Blanca et al., 2018)\",\n",
    "              \"to be sure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b876c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for outcome in outcome_li:\n",
    "    print(\"Checks for {}\".format(outcome))\n",
    "    print(\"Preset\")\n",
    "    anova_check_homoscedacity(outcome, \"preset_label\", tss_no_out)\n",
    "    print(\"\\nPrompt\")\n",
    "    anova_check_homoscedacity(outcome, \"prompt_label\", tss_no_out)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433b98e",
   "metadata": {},
   "source": [
    "## Running the ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf77aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_df = pd.DataFrame()\n",
    "\n",
    "# if heteroscedacity, run with fit(cov_type='HC3') and anova_lm(anova_mod, typ=2, robust='HC3')\n",
    "\n",
    "for outcome in outcome_li:\n",
    "    print(\"\\033[1m{}\\033[0m\".format(outcome))\n",
    "    ols_formula = outcome + \\\n",
    "        \" ~ C(preset_label, Sum) + C(prompt_label, Sum) + C(sample, Treatment(0))\"\n",
    "    ols_formula2 = outcome + \\\n",
    "        \" ~ C(preset_label, Sum(0)) + C(prompt_label, Sum(0)) + C(sample, Treatment(0))\"\n",
    "    anova_mod = ols(\n",
    "        ols_formula, tss_no_out).fit(cov_type='HC3')\n",
    "    anova_mod2 = ols(\n",
    "        ols_formula2, tss_no_out).fit(cov_type='HC3')\n",
    "    print(sm.stats.anova_lm(anova_mod, typ=2, robust='HC3'))\n",
    "    print(anova_mod.summary(alpha=0.1))\n",
    "\n",
    "    print(\"\\nDeviation contrasts for\\n{}:\\tcoef: {:.3f}\\tp: {:.3f}\"\n",
    "          .format(anova_mod2.params.index[7], anova_mod2.params[7], anova_mod2.pvalues[7]),\n",
    "          \"\\n{}:\\tcoef: {:.3f}\\tp: {:.3f}\"\n",
    "          .format(anova_mod2.params.index[10], anova_mod2.params[10], anova_mod2.pvalues[10]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    params_df[outcome] = anova_mod.params\n",
    "    params_df[outcome + \" p-value\"] = anova_mod.pvalues\n",
    "    params_df[outcome + \" 90% CI Lower\"] = anova_mod.conf_int(alpha=0.1)[0]\n",
    "\n",
    "    params_df.loc[anova_mod2.params.index[7], outcome] = anova_mod2.params[7]\n",
    "    params_df.loc[anova_mod2.params.index[7],\n",
    "                  outcome + \" p-value\"] = anova_mod2.pvalues[7]\n",
    "    params_df.loc[anova_mod2.params.index[7], outcome +\n",
    "                  \" 90% CI Lower\"] = anova_mod2.conf_int(alpha=0.1)[0][7]\n",
    "\n",
    "    params_df.loc[anova_mod2.params.index[10], outcome] = anova_mod2.params[10]\n",
    "    params_df.loc[anova_mod2.params.index[10],\n",
    "                  outcome + \" p-value\"] = anova_mod2.pvalues[10]\n",
    "    params_df.loc[anova_mod2.params.index[10], outcome +\n",
    "                  \" 90% CI Lower\"] = anova_mod2.conf_int(alpha=0.1)[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454db8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_df.index = params_df.index.str.replace(\n",
    "    r'C\\(.*[ST]\\.(.*)\\]', r'\\1', regex=True)\n",
    "\n",
    "# Transform 90% CI lower bound to 90% CI margin of error\n",
    "for outcome in outcome_li:\n",
    "    params_df[outcome + \" 90% CI Lower\"] = params_df[outcome] - \\\n",
    "        params_df[outcome + \" 90% CI Lower\"]\n",
    "params_df.columns = params_df.columns.str.replace(\n",
    "    \"90% CI Lower\", \"90% CI margin\", regex=False)\n",
    "\n",
    "presets_li = list(tss_no_out[\"preset_label\"].unique())\n",
    "prompts_li = list(tss_no_out[\"prompt_label\"].unique())\n",
    "\n",
    "order_index = [\"Intercept\"]\n",
    "order_index.extend(presets_li)\n",
    "order_index.extend(prompts_li)\n",
    "\n",
    "params_df = params_df.reindex(order_index)\n",
    "\n",
    "cols_order = []\n",
    "\n",
    "for outcome in outcome_li:\n",
    "    # adjusted ps for presets\n",
    "    params_df.loc[presets_li, outcome + \" adj_p\"] = fdrcorrection(\n",
    "        params_df.loc[presets_li, outcome + \" p-value\"], alpha=0.1)[1]\n",
    "    # adjusted ps for prompts\n",
    "    params_df.loc[prompts_li, outcome + \" adj_p\"] = fdrcorrection(\n",
    "        params_df.loc[prompts_li, outcome + \" p-value\"], alpha=0.1)[1]\n",
    "\n",
    "    cols_order.extend([outcome, outcome + \" p-value\",\n",
    "                       outcome + \" adj_p\", outcome + \" 90% CI margin\"])\n",
    "\n",
    "params_df = params_df[cols_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e261940",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_p_cols = params_df.columns.str.extractall(\"(.*adj_p)\").values.flatten()\n",
    "mask_sig = (params_df[adj_p_cols] < 0.1).any(axis=1)\n",
    "\n",
    "params_df[mask_sig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_no_out[\"preset_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [\"Coherence\", \"Creativity\", \"Avoiding Repitition\", \"Pace\"]\n",
    "\n",
    "# # create dicts with a key for each preset with an empty list as value for each\n",
    "# marginal_means_dict = {preset: [] for preset in params_df[1:8].index}\n",
    "# yerr_dict = {preset: [] for preset in params_df[1:8].index}\n",
    "# #colors = [\"tab:blue\", \"tab:green\", \"tab:red\", \"tab:orange\"]\n",
    "\n",
    "# for preset, outcome in itertools.product(marginal_means_dict.keys(), outcome_li):\n",
    "#     marginal_means_dict[preset].append(params_df.loc[\"Intercept\", outcome] + params_df.loc[preset, outcome])\n",
    "#     yerr_dict[preset].append(params_df.loc[preset, outcome + \" 90% CI margin\"])\n",
    "\n",
    "# figure = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# x = np.arange(len(labels)) # label locations\n",
    "# width = 0.35 # width of bars\n",
    "\n",
    "# plt.bar(x-width/2, marginal_means_dict[\"Genesis\"], width,\n",
    "#         yerr = yerr_dict[\"Genesis\"], label = \"Genesis\", color = \"tab:blue\")\n",
    "# plt.bar(x+width/2, marginal_means_dict[\"Ouroboros\"], width,\n",
    "#         yerr = yerr_dict[\"Ouroboros\"], label = \"Ouroboros\", color = \"tab:orange\")\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# pos = -0.4\n",
    "# for outcome in outcome_li:\n",
    "#     grand_mean = params_df.loc[\"Intercept\", outcome]\n",
    "#     ax = plt.plot([pos, pos+0.8], [grand_mean, grand_mean], '--', color = \"black\", alpha = 0.65)\n",
    "#     if i == 0:\n",
    "#         ax[0].set_label(\"Grand Mean (all Presets)\")\n",
    "#     i += 1\n",
    "#     pos += 1\n",
    "\n",
    "# plt.ylim(1,5)\n",
    "# plt.ylabel(\"Marginal Means (Community Sample)\", fontsize = 16)\n",
    "# plt.xticks(x, labels, fontsize = 16)\n",
    "\n",
    "# plt.legend(frameon=False, fontsize = 16, loc='upper center', ncol = 3)\n",
    "# leg = plt.gca().get_legend()\n",
    "# plt.grid(False)\n",
    "# #plt.title(\"Genesis & Low Rider\")\n",
    "# #figure.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# figure.savefig('graphs/gen_our_1.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb7ee5",
   "metadata": {},
   "source": [
    "## Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Coherence\", \"Creativity\",\n",
    "          \"Avoiding Repitition\", \"Pace\", \"Consistent Characters\"]\n",
    "presets_1 = [\"Genesis\", \"Ouroboros\",\n",
    "             \"Basic Coherence\", \"Low Rider\", \"All-Nighter\"]\n",
    "presets_2 = [\"Morpho\", \"Ace of Spade\"]\n",
    "\n",
    "preset_to_graph = [presets_1, presets_2]\n",
    "\n",
    "max_n_presets = max([len(x) for x in preset_to_graph])\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "# fig.tight_layout(pad=5.0)\n",
    "plt.setp(axes, ylim=(1, 5))\n",
    "fig.text(0.04, 0.5, \"Marginal Means (Community Sample)\",\n",
    "         va='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "i = 0\n",
    "for pre_li in preset_to_graph:\n",
    "\n",
    "    # create dicts with a key for each outcome with an empty list as value for each\n",
    "    marginal_means_dict = {outcome: [] for outcome in outcome_li}\n",
    "    yerr_dict = {outcome: [] for outcome in outcome_li}\n",
    "\n",
    "    prod_presets_outcomes = itertools.product(pre_li, outcome_li)\n",
    "\n",
    "    for preset, outcome in prod_presets_outcomes:\n",
    "        marginal_means_dict[outcome].append(\n",
    "            params_df.loc[\"Intercept\", outcome] + params_df.loc[preset, outcome])\n",
    "        yerr_dict[outcome].append(\n",
    "            params_df.loc[preset, outcome + \" 90% CI margin\"])\n",
    "\n",
    "    # Compute difference in length to longest row\n",
    "    diff_len = max_n_presets - len(pre_li)\n",
    "\n",
    "    # fill up dictonaries with 0 if less values than the longest row\n",
    "    if diff_len > 0:\n",
    "        for outcome in outcome_li:\n",
    "            marginal_means_dict[outcome].extend([0]*diff_len)\n",
    "            yerr_dict[outcome].extend([0]*diff_len)\n",
    "\n",
    "    x = np.arange(len(marginal_means_dict[\"tss_cre\"]))  # label locations\n",
    "    width = 0.15  # width of bars\n",
    "\n",
    "    cur_ax = axes[i]\n",
    "    n_presets = len(pre_li)\n",
    "\n",
    "    x_labels = pre_li\n",
    "    x_labels.extend([\"\"]*diff_len)\n",
    "\n",
    "    cur_ax.set_xticks(x)\n",
    "    cur_ax.set_xticklabels(x_labels, fontsize=16)\n",
    "\n",
    "    cur_ax.bar(x-0.3, marginal_means_dict[\"tss_coh\"], width,\n",
    "               yerr=yerr_dict[\"tss_coh\"], label=\"Coherence\", color=\"tab:blue\")\n",
    "\n",
    "    cur_ax.bar(x-0.15, marginal_means_dict[\"tss_cre\"], width,\n",
    "               yerr=yerr_dict[\"tss_cre\"], label=\"Creativity\", color=\"tab:green\")\n",
    "\n",
    "    cur_ax.bar(x, marginal_means_dict[\"tss_avoid_rep\"], width,\n",
    "               yerr=yerr_dict[\"tss_avoid_rep\"], label=\"Avoiding Repetition\", color=\"tab:red\")\n",
    "\n",
    "    cur_ax.bar(x+0.15, marginal_means_dict[\"tss_pac\"], width,\n",
    "               yerr=yerr_dict[\"tss_pac\"], label=\"Pace\", color=\"tab:orange\")\n",
    "\n",
    "    cur_ax.bar(x+0.3, marginal_means_dict[\"tss_conch\"], width,\n",
    "               yerr=yerr_dict[\"tss_pac\"], label=\"Consistent Characters\", color=\"tab:purple\")\n",
    "\n",
    "    outcome_i = 0\n",
    "    pos = -0.375\n",
    "    for outcome in outcome_li:\n",
    "        grand_mean = params_df.loc[\"Intercept\", outcome]\n",
    "        for preset_i in range(0, n_presets):\n",
    "            ax = cur_ax.plot([pos+preset_i, pos+width+preset_i],\n",
    "                             [grand_mean, grand_mean], '--', color=\"black\", alpha=0.7)\n",
    "        if outcome_i == 0:\n",
    "            ax[0].set_label(\"Grand Mean (all Presets)\")\n",
    "        outcome_i += 1\n",
    "        pos += width\n",
    "\n",
    "    handles, labels = cur_ax.get_legend_handles_labels()\n",
    "    order = [1, 2, 3, 4, 5, 0]\n",
    "    cur_ax.grid(False)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "axes[0].legend([handles[idx] for idx in order], [labels[idx] for idx in order],\n",
    "                  frameon=False, fontsize=16, loc='upper center', ncol=3, bbox_to_anchor=(0.6, 1))\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('graphs/gen_presets_preview.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef27f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph for prompts\n",
    "\n",
    "# create dicts with a key for each outcome with an empty list as value for each\n",
    "marginal_means_dict = {outcome: [] for outcome in outcome_li}\n",
    "yerr_dict = {outcome: [] for outcome in outcome_li}\n",
    "\n",
    "for prompt, outcome in itertools.product(prompts_li, outcome_li):\n",
    "    marginal_means_dict[outcome].append(\n",
    "        params_df.loc[\"Intercept\", outcome] + params_df.loc[prompt, outcome])\n",
    "    yerr_dict[outcome].append(\n",
    "        params_df.loc[prompt, outcome + \" 90% CI margin\"])\n",
    "\n",
    "figure = plt.figure(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(marginal_means_dict[\"tss_cre\"]))  # label locations\n",
    "width = 0.15  # width of bars\n",
    "\n",
    "plt.bar(x-0.3, marginal_means_dict[\"tss_coh\"], width,\n",
    "        yerr=yerr_dict[\"tss_coh\"], label=\"Coherence\", color=\"tab:blue\")\n",
    "\n",
    "plt.bar(x-0.15, marginal_means_dict[\"tss_cre\"], width,\n",
    "        yerr=yerr_dict[\"tss_cre\"], label=\"Creativity\", color=\"tab:green\")\n",
    "\n",
    "plt.bar(x, marginal_means_dict[\"tss_avoid_rep\"], width,\n",
    "        yerr=yerr_dict[\"tss_avoid_rep\"], label=\"Avoiding Repetition\", color=\"tab:red\")\n",
    "\n",
    "plt.bar(x+0.15, marginal_means_dict[\"tss_pac\"], width,\n",
    "        yerr=yerr_dict[\"tss_pac\"], label=\"Pace\", color=\"tab:orange\")\n",
    "\n",
    "plt.bar(x+0.3, marginal_means_dict[\"tss_conch\"], width,\n",
    "        yerr=yerr_dict[\"tss_pac\"], label=\"Consistent Characters\", color=\"tab:purple\")\n",
    "\n",
    "plt.ylim(1, 5)\n",
    "plt.ylabel(\"Marginal Means (Community Sample)\", fontsize=16)\n",
    "plt.xticks(x, prompts_li, fontsize=16)\n",
    "\n",
    "outcome_i = 0\n",
    "pos = -0.375\n",
    "for outcome in outcome_li:\n",
    "    grand_mean = params_df.loc[\"Intercept\", outcome]\n",
    "    for preset_i in range(0, len(plt.gca().get_xticks())):\n",
    "        ax = plt.plot([pos+preset_i, pos+width+preset_i],\n",
    "                      [grand_mean, grand_mean], '--', color=\"black\", alpha=0.7)\n",
    "    if outcome_i == 0:\n",
    "        ax[0].set_label(\"Grand Mean (all Prompts)\")\n",
    "    outcome_i += 1\n",
    "    pos += width\n",
    "\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "order = [1, 2, 3, 4, 5, 0]\n",
    "plt.legend([handles[idx] for idx in order], [labels[idx] for idx in order],\n",
    "           frameon=False, fontsize=16, loc='upper center', ncol=3, bbox_to_anchor=(0.6, 1))\n",
    "# plt.legend(frameon=False, bbox_to_anchor = (1,.9), fontsize = 16)\n",
    "plt.grid(False)\n",
    "#plt.title(\"Genesis & Low Rider\")\n",
    "# figure.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "figure.savefig('graphs/gen_prompts.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd372074",
   "metadata": {},
   "outputs": [],
   "source": [
    "genesis_marginal_means = []\n",
    "genesis_yerr = []\n",
    "colors = [\"tab:blue\", \"tab:green\", \"tab:red\", \"tab:orange\", \"tab:purple\"]\n",
    "\n",
    "for outcome in outcome_li:\n",
    "    genesis_marginal_means.append(\n",
    "        params_df.loc[\"Intercept\", outcome] + params_df.loc[\"Genesis\", outcome])\n",
    "    genesis_yerr.append(params_df.loc[\"Genesis\", outcome + \" 90% CI margin\"])\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.bar([\"Coherence\", \"Creativity\", \"Avoiding Repitition\", \"Pace\", \"Consistent Char.\"],\n",
    "        genesis_marginal_means, yerr=genesis_yerr,\n",
    "        color=colors)\n",
    "\n",
    "i = 0\n",
    "pos = -0.5\n",
    "for outcome in outcome_li:\n",
    "    grand_mean = params_df.loc[\"Intercept\", outcome]\n",
    "    ax = plt.plot([pos, pos+1], [grand_mean, grand_mean],\n",
    "                  '--', color=colors[i])\n",
    "    if i == 0:\n",
    "        ax[0].set_label(\"Grand Mean (all Presets)\")\n",
    "    i += 1\n",
    "    pos += 1\n",
    "\n",
    "plt.ylim(1, 5)\n",
    "plt.ylabel(\"Marginal Means (Community Sample)\")\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "leg = plt.gca().get_legend()\n",
    "leg.legendHandles[0].set_color('black')\n",
    "plt.grid(False)\n",
    "plt.title(\"Genesis Preset\")\n",
    "plt.show()\n",
    "figure.savefig('graphs/genesis.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0310f1c",
   "metadata": {},
   "source": [
    "# Analyze Consistency of Story Aspects Across Presets & Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af0e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for outcome in outcome_li:\n",
    "    print((\"_\"*75) + \"\\n\")\n",
    "    print(\"\\033[1m{}\\033[0m\".format(outcome))\n",
    "\n",
    "    homo_res = pg.homoscedasticity(\n",
    "        tss_no_out, dv=outcome, group=\"preset_label\", method=\"levene\", alpha=0.1)\n",
    "    print(\"Omnibus Test for homoscedasticity across presets\")\n",
    "    print(homo_res.round(3))\n",
    "\n",
    "    if homo_res.iloc[0, 2] == False:\n",
    "        print(\"\\nVariances are not equal, doing posthoc tests\")\n",
    "        print(f\"Average SD: {tss_no_out[outcome].std():.2f}\")\n",
    "        print(\"\\nPresets with adjusted p-vals < .15 displayed below:\")\n",
    "        \n",
    "        posthoc_dict = {}\n",
    "        for preset in presets_li:\n",
    "            data_preset = tss_no_out[tss_no_out[\"preset_label\"]\n",
    "                                     == preset][outcome].to_numpy()\n",
    "            data_rest = tss_no_out[tss_no_out[\"preset_label\"]\n",
    "                                   != preset][outcome].to_numpy()\n",
    "            posthoc_dict[preset] = pg.homoscedasticity(\n",
    "                [data_preset, data_rest], method=\"levene\", alpha=0.1)\n",
    "\n",
    "        ps_li = []\n",
    "        for dv in posthoc_dict:\n",
    "            ps_li.append(posthoc_dict[dv].iloc[0, 1])\n",
    "        adj_ps_li = fdrcorrection(ps_li, alpha=0.1)[1]\n",
    "\n",
    "        i = 0\n",
    "        for dv in posthoc_dict:\n",
    "            posthoc_dict[dv].loc[\"levene\", \"sd\"] = tss_no_out[tss_no_out[\"preset_label\"] == dv][outcome].std()\n",
    "            posthoc_dict[dv].loc[\"levene\", \"adj_p\"] = adj_ps_li[i]\n",
    "            if posthoc_dict[dv].loc[\"levene\", \"adj_p\"] < 0.15:\n",
    "                print(f\"\\n{dv}\")\n",
    "                print(posthoc_dict[dv].round(3))\n",
    "            i += 1\n",
    "\n",
    "    homo_res = pg.homoscedasticity(\n",
    "        tss_no_out, dv=outcome, group=\"prompt_label\", method=\"levene\", alpha=0.1)\n",
    "    print(\"-\"*75)\n",
    "    print(\"\\nOmnibus Test for homoscedasticity across prompts\")\n",
    "    print(f\"{homo_res.round(3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7844b",
   "metadata": {},
   "source": [
    "## Visualize Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edafe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_over_groups(df, var, var_name, group_label, groups_li,\n",
    "                     bins_n=10, xlim= (1,5), size=(15, 15),\n",
    "                    plot_avg=True):\n",
    "    n_plots = len(groups_li)+1 if plot_avg else len(groups_li)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=size)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    x = 0\n",
    "    \n",
    "    if plot_avg:\n",
    "        df[var].plot.hist(ax=axes[x], bins=bins_n)\n",
    "        df[var].plot.kde(ax=axes[x], secondary_y=True)\n",
    "        axes[x].set_xlim(xlim)\n",
    "        axes[x].set_title(f\"{var_name} - Average\")\n",
    "        plt.grid(False)\n",
    "        x += 1\n",
    "        \n",
    "    for group in groups_li:\n",
    "        df_group = df[df[group_label]==group]\n",
    "        df_group[var].plot.hist(ax=axes[x], bins=bins_n)\n",
    "        df_group[var].plot.kde(ax=axes[x], secondary_y=True)\n",
    "        axes[x].set_xlim(xlim)\n",
    "        axes[x].set_title(f\"{var_name} - {group}\")\n",
    "        plt.grid(False)\n",
    "        x += 1\n",
    "        \n",
    "def box_over_groups(df, var, var_name, group_label, groups_li,\n",
    "                     ylim= (1,5), size=(15, 15),\n",
    "                    plot_avg=True, save_plot = False):\n",
    "    n_plots = len(groups_li)+1 if plot_avg else len(groups_li)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=size)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    x = 0\n",
    "    \n",
    "    if plot_avg:\n",
    "        sns.boxplot(y=df[var], ax=axes[x])\n",
    "        axes[x].set_ylim(ylim)\n",
    "        axes[x].set_title(f\"{var_name} - Average\")\n",
    "        axes[x].grid(False)\n",
    "        x += 1\n",
    "        \n",
    "    for group in groups_li:\n",
    "        df_group = df[df[group_label]==group]\n",
    "        sns.boxplot(y=df_group[var], ax=axes[x])\n",
    "        axes[x].set_ylim(ylim)\n",
    "        axes[x].set_title(f\"{var_name} - {group}\")\n",
    "        axes[x].grid(False)\n",
    "        x += 1\n",
    "        \n",
    "    if save_plot:\n",
    "        fig.savefig(save_plot, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_over_groups(tss_no_out, \"tss_coh\", \"Coherence\",\n",
    "                 \"preset_label\", [\"All-Nighter\", \"Basic Coherence\"],\n",
    "                size=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_over_groups(tss_no_out, \"tss_coh\", \"Coherence\",\n",
    "                 \"preset_label\", [\"All-Nighter\", \"Basic Coherence\"],\n",
    "                size=(15,6), save_plot = \"graphs/box_coh.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5eab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hist_over_groups(tss_no_out, \"tss_pac\", \"Coherence\",\n",
    "#                  \"preset_label\", [\"Low Rider\", \"Ace of Spade\", \"Morpho\"],\n",
    "#                 size=(15,5), bins_n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9928ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_over_groups(tss_no_out, \"tss_pac\", \"Pace\",\n",
    "#                  \"preset_label\", [\"Low Rider\", \"Ace of Spade\", \"Morpho\"],\n",
    "#                 size=(15,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_no_out[[\"tss_coh\", \"tss_avoid_rep\", \"tss_pac\", \"tss_cre\", \"tss_conch\",\n",
    "            \"tss_cre_4\",\n",
    "            \"tss_qua_1\", \"tss_qua_2\"]].corr(method=\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed087e25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "items_descr[[\"tss_cre_4\", \"tss_qua_1\", \"tss_qua_2\", \"tss_qua_3\",\n",
    "             \"tss_qua_5\", \"tss_qua_6\", \"tss_qua_7\", \"tss_qua_8\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f0e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tss_no_out[\"qual_2_agree+\"] = tss_no_out[\"tss_qua_2\"].apply(\n",
    "    lambda x: 1 if x >= 4 else 0)\n",
    "tss_no_out[\"qual_1_agree+\"] = tss_no_out[\"tss_qua_1\"].apply(\n",
    "    lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870beb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center preds and prepare polynomial arrays\n",
    "unc_pred_names_li = [\"word_count\", \"tss_coh\",\n",
    "                     \"tss_cre\", \"tss_avoid_rep\", \"tss_pac\", \"tss_conch\"]\n",
    "cent_pred_names_li = []\n",
    "\n",
    "# Defining predictors for models\n",
    "for pred in unc_pred_names_li:\n",
    "    cent_pred = pred + \"_cent\"\n",
    "    tss_no_out[cent_pred] = tss_no_out[pred].apply(\n",
    "        lambda x: x-tss_no_out[pred].mean())  # centering\n",
    "    cent_pred_names_li.append(cent_pred)\n",
    "\n",
    "lin_pred_data = tss_no_out[cent_pred_names_li].to_numpy()\n",
    "\n",
    "poly_lin = polyfeat(interaction_only=True, include_bias=False)\n",
    "lin_int_pred_data = poly_lin.fit_transform(lin_pred_data)\n",
    "\n",
    "quad_pred_data = np.hstack((lin_pred_data, lin_pred_data**2))\n",
    "cub_pred_data = np.hstack((lin_pred_data, lin_pred_data**2, lin_pred_data**3))\n",
    "quart_pred_data = np.hstack(\n",
    "    (lin_pred_data, lin_pred_data**2, lin_pred_data**3, lin_pred_data**4))\n",
    "\n",
    "# Setting up k-fold cross validation\n",
    "kf = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "log_model = logreg(max_iter=1000)\n",
    "\n",
    "# setup list of model names with predictor arrays\n",
    "poly_preds_li = [(\"linear\", lin_pred_data), (\"linear with interactions\", lin_int_pred_data),\n",
    "                 (\"quadratic\", quad_pred_data),\n",
    "                 (\"cubic\", cub_pred_data), (\"quartic\", quart_pred_data)]\n",
    "\n",
    "qua_li = [\"qual_1_agree+\", \"qual_2_agree+\"]\n",
    "\n",
    "for qual_measure in qua_li:\n",
    "    for poly_pred in poly_preds_li:\n",
    "        y = tss_no_out[qual_measure]\n",
    "        current_pred = poly_pred[1]\n",
    "        current_model_name = poly_pred[0]\n",
    "        cross_val = cross_validate(log_model, current_pred, y,\n",
    "                                   scoring=[\"neg_log_loss\"], cv=kf)\n",
    "        # get negative loss scores, throw out nans if we got some\n",
    "        nll = cross_val[\"test_neg_log_loss\"]\n",
    "        nll = nll[~np.isnan(nll)]\n",
    "        avg_nll = np.mean(nll)\n",
    "        print(\"Model performance for {} model predicting good or better output for {}:\".format(\n",
    "            current_model_name, qual_measure))\n",
    "        print(\"negative log loss:{:.4f}\\n\".format(avg_nll))\n",
    "    print(\"\")\n",
    "\n",
    "    # linear without interactions performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_q1 = smf.logit((\"Q('qual_1_agree+') ~ word_count_cent + tss_coh_cent\" +\n",
    "                        \"+ tss_cre_cent + tss_avoid_rep_cent + tss_pac_cent\" +\n",
    "                        \"+ tss_conch_cent\" +\n",
    "                        \"+ C(sample, Treatment(0))\"),\n",
    "                       data=tss_no_out).fit()\n",
    "log_reg_q2 = smf.logit((\"Q('qual_2_agree+') ~ word_count_cent + tss_coh_cent\" +\n",
    "                        \"+ tss_cre_cent + tss_avoid_rep_cent + tss_pac_cent\" +\n",
    "                        \"+ tss_conch_cent\" +\n",
    "                        \"+ C(sample, Treatment(0))\"),\n",
    "                       data=tss_no_out).fit()\n",
    "\n",
    "print(\"\\n\\033[1mQuality Item 1 (Enjoyment)\\033[0m\")\n",
    "print(log_reg_q1.summary())\n",
    "print(\"\\n\\033[1mQuality Item 2 (High Quality)\\033[0m\")\n",
    "print(log_reg_q2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_li = [(\"Quality 1 (Enjoyment)\", log_reg_q1),\n",
    "             (\"Quality 2 (High Quality)\", log_reg_q2)]\n",
    "\n",
    "for log_reg in logreg_li:\n",
    "    print(f\"Predictions & Performance for {log_reg[0]}\")\n",
    "    pred_table = log_reg[1].pred_table()\n",
    "\n",
    "    # read out table values\n",
    "    tn = pred_table[0, 0]\n",
    "    tp = pred_table[1, 1]\n",
    "    fn = pred_table[1, 0]\n",
    "    fp = pred_table[0, 1]\n",
    "    total = np.sum(pred_table)\n",
    "\n",
    "    # Observed Probability\n",
    "    obs_p = (tp + fn) / total\n",
    "\n",
    "    # predicted prob\n",
    "    pred_prob = (tp + fp) / total\n",
    "\n",
    "    # Accuracy: Correct classification over all cases\n",
    "    acc = (tp + tn) / total\n",
    "\n",
    "    # Precision: Accuracy for data predicted to be positive\n",
    "    prec = tp / (tp+fp)\n",
    "    # Recall: Accuracy for positive (enjoyable/high quality) data\n",
    "    rec = tp / (tp+fn)\n",
    "\n",
    "    odds_ratio = np.exp(log_reg[1].params)\n",
    "\n",
    "    print(f\"Observed Probability: {obs_p:.2f}\\n\"\n",
    "          f\"Predicted Probability: {pred_prob:.2f}\\n\\n\"\n",
    "          f\"Model Performance\\n\"\n",
    "          f\"Accuracy: {acc:.2f}\\n\"\n",
    "          f\"Precision: {prec:.2f}\\n\"\n",
    "          f\"Recall: {rec:.2f}\\n\"\n",
    "          f\"\\nOdds Ratio:\\n{odds_ratio}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10695436",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_df_template = pd.DataFrame()\n",
    "pred_df_template[\"plot_values\"] = np.arange(1.0, 5.1, 0.1)\n",
    "pred_df_template[[\"tss_coh_cent\", \"tss_cre_cent\",\n",
    "                  \"tss_avoid_rep_cent\", \"tss_pac_cent\", \"tss_conch_cent\",\n",
    "                  \"word_count_cent\"]] = 0\n",
    "pred_df_template[\"sample\"] = \"Community\"\n",
    "\n",
    "pred_vars = [(\"Coherence\", \"tss_coh\", \"tab:blue\"),\n",
    "             (\"Creativity\", \"tss_cre\", \"tab:green\"),\n",
    "             (\"Avoid Repetition\", \"tss_avoid_rep\", \"tab:red\"),\n",
    "             (\"Pace\", \"tss_pac\", \"tab:orange\"),\n",
    "             (\"Consistent Characters\", \"tss_conch\", \"tab:purple\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "for var in pred_vars:\n",
    "    # Construct prediction df\n",
    "    pred_df = pred_df_template.copy()\n",
    "    var_full = var[0]\n",
    "    var_short = var[1]\n",
    "    color = var[2]\n",
    "    pred_df.rename(columns={\"plot_values\": var_full}, inplace=True)\n",
    "    pred_df[var_short + \"_cent\"] = pred_df[var_full] - \\\n",
    "        tss_no_out[var_short].mean()\n",
    "    pred_df[\"Pred. Prob Qual 1\"] = log_reg_q1.predict(pred_df)\n",
    "\n",
    "    pred_plot = sns.regplot(data=pred_df, y=\"Pred. Prob Qual 1\",\n",
    "                            x=var_full, scatter=False, ax=ax, label=var_full,\n",
    "                            x_ci=None, lowess=True, color=color)\n",
    "\n",
    "plt.grid(False)\n",
    "#plt.xlim(100, 2000)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"Predicted Probability for Agree+\\n(for Community Sample)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "ax.legend(fontsize=18, frameon=False, bbox_to_anchor=(.7, .6))\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "ax.xaxis.label.set_fontsize(18)\n",
    "ax.yaxis.label.set_fontsize(18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Chance for Enjoyable Story\", fontsize=20)\n",
    "plt.savefig(\"graphs/logreg_qual_1.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d207dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_template = pd.DataFrame()\n",
    "pred_df_template[\"plot_values\"] = np.arange(1.0, 5.1, 0.1)\n",
    "pred_df_template[[\"tss_coh_cent\", \"tss_cre_cent\",\n",
    "                  \"tss_avoid_rep_cent\", \"tss_pac_cent\",\n",
    "                  \"tss_conch_cent\",\n",
    "                  \"word_count_cent\"]] = 0\n",
    "pred_df_template[\"sample\"] = \"Community\"\n",
    "\n",
    "pred_vars = [(\"Coherence\", \"tss_coh\", \"tab:blue\"),\n",
    "             (\"Creativity\", \"tss_cre\", \"tab:green\"),\n",
    "             (\"Avoid Repetition\", \"tss_avoid_rep\", \"tab:red\"),\n",
    "             (\"Pace\", \"tss_pac\", \"tab:orange\"),\n",
    "             (\"Consistent Characters\", \"tss_conch\", \"tab:purple\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "for var in pred_vars:\n",
    "    # Construct prediction df\n",
    "    pred_df = pred_df_template.copy()\n",
    "    var_full = var[0]\n",
    "    var_short = var[1]\n",
    "    color = var[2]\n",
    "    pred_df.rename(columns={\"plot_values\": var_full}, inplace=True)\n",
    "    pred_df[var_short + \"_cent\"] = pred_df[var_full] - \\\n",
    "        tss_no_out[var_short].mean()\n",
    "    pred_df[\"Pred. Prob Qual 2\"] = log_reg_q2.predict(pred_df)\n",
    "\n",
    "    pred_plot = sns.regplot(data=pred_df, y=\"Pred. Prob Qual 2\",\n",
    "                            x=var_full, scatter=False, ax=ax, label=var_full,\n",
    "                            x_ci=None, lowess=True, color=color)\n",
    "\n",
    "plt.grid(False)\n",
    "#plt.xlim(100, 2000)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"Predicted Probability for Agree+\")\n",
    "plt.xlabel(\"Rating\")\n",
    "ax.legend(fontsize=18, frameon=False, bbox_to_anchor=(.7, .9))\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "ax.xaxis.label.set_fontsize(18)\n",
    "ax.yaxis.label.set_fontsize(18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Chance for High-Quality Story\", fontsize=20)\n",
    "plt.savefig(\"graphs/logreg_qual_2.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_q1 = smf.logit((\"Q('qual_1_agree+') ~  C(preset_label, Sum) + C(prompt_label, Sum)\" +\n",
    "                        \"+ C(sample, Treatment(0))\"),\n",
    "                       data=tss_no_out).fit()\n",
    "log_reg_q2 = smf.logit((\"Q('qual_2_agree+') ~ C(preset_label, Sum) + C(prompt_label, Sum)\" +\n",
    "                        \"+ C(sample, Treatment(0))\"),\n",
    "                       data=tss_no_out).fit()\n",
    "\n",
    "print(\"\\n\\033[1mQuality Item 1 (Enjoyment)\\033[0m\")\n",
    "print(log_reg_q1.summary())\n",
    "print(\"\\n\\033[1mQuality Item 2 (High Quality)\\033[0m\")\n",
    "print(log_reg_q2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qual = tss_no_out.groupby('preset_label').mean(\n",
    "# )[[\"tss_qua_2\", \"tss_qua_1\", \"tss_qua_3\"]].sort_values(\"tss_qua_1\")\n",
    "\n",
    "# figure = plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x = \"tss_qua_1\", y = \"preset_label\", data = tss_no_out)\n",
    "#plt.bar(qual.index, qual[\"tss_qua_1\"])\n",
    "#plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b863f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 2, figsize=(10, 30))\n",
    "fig.tight_layout(pad=5.0)\n",
    "plt.grid(False)\n",
    "\n",
    "sns.regplot(data=tss_no_out, y=\"tss_coh\",\n",
    "            x=\"read_consesus\", lowess=True, ax=axes[0, 0])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_coh\",\n",
    "            x=\"read_fre\", lowess=True, ax=axes[0, 1])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_cre\",\n",
    "            x=\"read_consesus\", lowess=True, ax=axes[1, 0])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_cre\",\n",
    "            x=\"read_fre\", lowess=True, ax=axes[1, 1])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_avoid_rep\",\n",
    "            x=\"read_consesus\", lowess=True, ax=axes[2, 0])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_avoid_rep\",\n",
    "            x=\"read_fre\", lowess=True, ax=axes[2, 1])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_pac\",\n",
    "            x=\"read_consesus\", lowess=True, ax=axes[3, 0])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_pac\",\n",
    "            x=\"read_fre\", lowess=True, ax=axes[3, 1])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_conch\",\n",
    "            x=\"read_consesus\", lowess=True, ax=axes[4, 0])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_conch\",\n",
    "            x=\"read_fre\", lowess=True, ax=axes[4, 1])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_qua_1\",\n",
    "            x=\"read_consesus\", lowess=True, ax=axes[5, 0])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_qua_1\",\n",
    "            x=\"read_fre\", lowess=True, ax=axes[5, 1])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_qua_2\",\n",
    "            x=\"read_consesus\", lowess=True, ax=axes[6, 0])\n",
    "sns.regplot(data=tss_no_out, y=\"tss_qua_2\",\n",
    "            x=\"read_fre\", lowess=True, ax=axes[6, 1])\n",
    "for ax in fig.axes:\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8809bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up k-fold cross validation\n",
    "\n",
    "kf = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining predictors for models\n",
    "tss_df[\"read_cent\"] = tss_df[\"read_consesus\"].apply(\n",
    "    lambda x: x-tss_df[\"read_consesus\"].mean())  # centering\n",
    "\n",
    "tss_df[\"read_cent**2\"] = tss_df[\"read_cent\"]*tss_df[\"read_cent\"]\n",
    "tss_df[\"read_cent**3\"] = tss_df[\"read_cent\"]**3\n",
    "tss_df[\"read_cent**4\"] = tss_df[\"read_cent\"]**4\n",
    "tss_df[\"read_cent**5\"] = tss_df[\"read_cent\"]**5\n",
    "\n",
    "lin_model = linreg()\n",
    "\n",
    "pred_li = [(\"linear\", \"read_cent\"), (\"quadratic\", \"read_cent**2\"),\n",
    "           (\"cubic\", \"read_cent**3\"), (\"quartic\", \"read_cent**4\"),\n",
    "           (\"quintic\", \"read_cent**5\")]\n",
    "outcome_li = [\"tss_coh\", \"tss_cre\", \"tss_avoid_rep\",\n",
    "              \"tss_pac\", \"tss_conch\", \"tss_qua_1\", \"tss_qua_2\"]\n",
    "\n",
    "\n",
    "for outcome in outcome_li:\n",
    "    y = tss_df[outcome]\n",
    "    current_preds_col = []\n",
    "\n",
    "    for pred in pred_li:\n",
    "        current_preds_col.append(pred[1])\n",
    "        x = tss_df.loc[:, current_preds_col]\n",
    "\n",
    "        current_model_name = pred[0]\n",
    "\n",
    "        cross_val = cross_validate(lin_model, x, y,\n",
    "                                   scoring=[\"neg_mean_squared_error\", \"r2\"], cv=kf)\n",
    "\n",
    "        neg_mses = cross_val[\"test_neg_mean_squared_error\"]\n",
    "        r_squares = cross_val[\"test_r2\"]\n",
    "        avg_rmse = np.mean((neg_mses*-1)**0.5)\n",
    "        avg_r_sq = np.mean(r_squares)\n",
    "        print(\"Model performance for {} model predicting {}:\".format(\n",
    "            current_model_name, outcome))\n",
    "        print(\"r-square: {:.4f}    RMSE: {:.4f}\".format(avg_r_sq, avg_rmse))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_li = [\"tss_coh\", \"tss_cre\", \"tss_avoid_rep\", \"tss_pac\", \"tss_conch\"]\n",
    "\n",
    "marg_means_df = params_df.loc[[\"Intercept\", \"Genesis\", \"Ouroboros\", \"Basic Coherence\", \"Low Rider\",\n",
    "                               \"All-Nighter\", \"Morpho\", \"Ace of Spade\"],\n",
    "                              [\"tss_coh\", \"tss_cre\", \"tss_avoid_rep\", \"tss_pac\", \"tss_conch\",\n",
    "                               \"tss_coh 90% CI margin\", \"tss_cre 90% CI margin\",\n",
    "                               \"tss_avoid_rep 90% CI margin\", \"tss_pac 90% CI margin\",\n",
    "                               \"tss_conch 90% CI margin\",\n",
    "                               \"tss_coh adj_p\", \"tss_cre adj_p\",\n",
    "                               \"tss_avoid_rep adj_p\", \"tss_pac adj_p\",\n",
    "                               \"tss_conch adj_p\"]].copy()\n",
    "\n",
    "for outcome in outcome_li:\n",
    "    marg_means_df.loc[\n",
    "        [\"Genesis\", \"Ouroboros\", \"Basic Coherence\", \"Low Rider\", \"All-Nighter\", \"Morpho\", \"Ace of Spade\"], outcome] = marg_means_df.loc[\n",
    "        [\"Genesis\", \"Ouroboros\", \"Basic Coherence\", \"Low Rider\", \"All-Nighter\", \"Morpho\", \"Ace of Spade\"], outcome].apply(\n",
    "        lambda x: marg_means_df.loc[\"Intercept\", outcome]+x)\n",
    "\n",
    "marg_means_df.rename(index={\"Intercept\": \"Grand Mean\"}, inplace=True)\n",
    "marg_means_df.columns = marg_means_df.columns.str.replace(\n",
    "    \"tss_coh\", \"Coherence\")\n",
    "marg_means_df.columns = marg_means_df.columns.str.replace(\n",
    "    \"tss_cre\", \"Creativity\")\n",
    "marg_means_df.columns = marg_means_df.columns.str.replace(\n",
    "    \"tss_avoid_rep\", \"Avoiding Repitition\")\n",
    "marg_means_df.columns = marg_means_df.columns.str.replace(\"tss_pac\", \"Pace\")\n",
    "marg_means_df.columns = marg_means_df.columns.str.replace(\n",
    "    \"tss_conch\", \"Consistent Characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "marg_means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update GSheet from Basileus\n",
    "gc = gs.oauth()\n",
    "\n",
    "gsheet = gc.open(\"Preset Analysis\").worksheet(\"Data\")\n",
    "gsheet_df = gd.get_as_dataframe(gsheet, index_col=\"Preset\")\n",
    "gsheet_df.update(marg_means_df)\n",
    "gd.set_with_dataframe(gsheet, gsheet_df, include_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((tss_no_out[\"preset_label\"] == \"Morpho\") &\n",
    "        (tss_no_out[\"tss_coh\"] > 2.55) &\n",
    "        (tss_no_out[\"tss_avoid_rep\"] > 2.21) &\n",
    "        (tss_no_out[\"tss_avoid_rep\"] < 2.70) &\n",
    "        (tss_no_out[\"sample\"] == \"Community\")\n",
    "        )\n",
    "\n",
    "tss_no_out[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94641cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tss_no_out.loc[35, outcome_li])\n",
    "print(tss_no_out.loc[35, \"full_story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tss_no_out.loc[130, outcome_li])\n",
    "print(tss_no_out.loc[130, \"full_story\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf62433",
   "metadata": {},
   "source": [
    "| Preset | Performance during continuous story generation<br />(Without user intervention) |\n",
    "|:-|-|\n",
    "| Genesis | Less coherent than the average |\n",
    "| Basic Coherence | Narrow range in terms of Coherence.<br />Most outputs are close to average Coherence.<br />(= Low variance) |\n",
    "| Ouroboros, Low Rider | Solid - average on all story aspects |\n",
    "| Ace of Spade | Less repetitions than the average |\n",
    "| All-Nighter | Wide range of possible outputs in terms of Coherence.<br />You'll get more incoherent outputs, but also more strongly coherent ones.<br />(= High Variance) |\n",
    "| Morpho | Major issues with repetitions<br />Low pace (possibly due to repetitions) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "547.85px",
    "left": "1163.2px",
    "right": "20px",
    "top": "120px",
    "width": "336px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
